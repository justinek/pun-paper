%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proceedings of the National Academy of Sciences (PNAS)
% LaTeX Template
% Version 1.0 (19/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% The PNAStwo class was created and is owned by PNAS:
% http://www.pnas.org/site/authors/LaTex.xhtml
% This template has been modified from the blank PNAS template to include
% examples of how to insert content and drastically change commenting. The
% structural integrity is maintained as in the original blank template.
%
% Original header:
%% PNAStmpl.tex
%% Template file to use for PNAS articles prepared in LaTeX
%% Version: Apr 14, 2008
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

%------------------------------------------------
% BASIC CLASS FILE
%------------------------------------------------

%% PNAStwo for two column articles is called by default.
%% Uncomment PNASone for single column articles. One column class
%% and style files are available upon request from pnas@nas.edu.

%\documentclass{pnasone}
\documentclass{pnastwo}

%------------------------------------------------
% POSITION OF TEXT
%------------------------------------------------

%% Changing position of text on physical page:
%% Since not all printers position
%% the printed page in the same place on the physical page,
%% you can change the position yourself here, if you need to:

% \advance\voffset -.5in % Minus dimension will raise the printed page on the 
                         %  physical page; positive dimension will lower it.

%% You may set the dimension to the size that you need.

%------------------------------------------------
% GRAPHICS STYLE FILE
%------------------------------------------------

%% Requires graphics style file (graphicx.sty), used for inserting
%% .eps/image files into LaTeX articles.
%% Note that inclusion of .eps files is for your reference only;
%% when submitting to PNAS please submit figures separately.

%% Type into the square brackets the name of the driver program 
%% that you are using. If you don't know, try dvips, which is the
%% most common PC driver, or textures for the Mac. These are the options:

% [dvips], [xdvi], [dvipdf], [dvipdfm], [dvipdfmx], [pdftex], [dvipsone],
% [dviwindo], [emtex], [dviwin], [pctexps], [pctexwin], [pctexhp], [pctex32],
% [truetex], [tcidvi], [vtex], [oztex], [textures], [xetex]


%------------------------------------------------
% OPTIONAL POSTSCRIPT FONT FILES
%------------------------------------------------

%% PostScript font files: You may need to edit the PNASoneF.sty
%% or PNAStwoF.sty file to make the font names match those on your system. 
%% Alternatively, you can leave the font style file commands commented out
%% and typeset your article using the default Computer Modern 
%% fonts (recommended). If accepted, your article will be typeset
%% at PNAS using PostScript fonts.

% Choose PNASoneF for one column; PNAStwoF for two column:
%\usepackage{PNASoneF}
%\usepackage{PNAStwoF}

%------------------------------------------------
% ADDITIONAL OPTIONAL STYLE FILES
%------------------------------------------------

%% The AMS math files are commonly used to gain access to useful features
%% like extended math fonts and math commands.
%\usepackage{url}
\usepackage{multirow}
\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{etex}
\usepackage{color}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{decorations.shapes}
\usepackage{amssymb,amsfonts,amsmath}


%------------------------------------------------
% OPTIONAL MACRO FILES
%------------------------------------------------

%% Insert self-defined macros here.
%% \newcommand definitions are recommended; \def definitions are supported

%\newcommand{\mfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
%\def\s{\sigma}

%------------------------------------------------
% DO NOT EDIT THIS SECTION
%------------------------------------------------

%% For PNAS Only:
\contributor{Submitted to Proceedings of the National Academy of Sciences of the United States of America}
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHORS
%----------------------------------------------------------------------------------------

\title{A computational model of linguistic humor in puns} % For titles, only capitalize the first letter

%------------------------------------------------

%% Enter authors via the \author command.  
%% Use \affil to define affiliations.
%% (Leave no spaces between author name and \affil command)

%% Note that the \thanks{} command has been disabled in favor of
%% a generic, reserved space for PNAS publication footnotes.

%% \author{<author name>
%% \affil{<number>}{<Institution>}} One number for each institution.
%% The same number should be used for authors that
%% are affiliated with the same institution, after the first time
%% only the number is needed, ie, \affil{number}{text}, \affil{number}{}
%% Then, before last author ...
%% \and
%% \author{<author name>
%% \affil{<number>}{}}

%% For example, assuming Garcia and Sonnery are both affiliated with
%% Universidad de Murcia:
%% \author{Roberta Graff\affil{1}{University of Cambridge, Cambridge,
%% United Kingdom},
%% Javier de Ruiz Garcia\affil{2}{Universidad de Murcia, Bioquimica y Biologia
%% Molecular, Murcia, Spain}, \and Franklin Sonnery\affil{2}{}}

\author{Justine T. Kao\affil{1}{Stanford University},
Roger Levy\affil{2}{University of California, San Diego}
\and
Noah D. Goodman\affil{1}{}}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

%----------------------------------------------------------------------------------------

\maketitle % The \maketitle command is necessary to build the title page

\begin{article}

%----------------------------------------------------------------------------------------
%	ABSTRACT, KEYWORDS AND ABBREVIATIONS
%----------------------------------------------------------------------------------------

\begin{abstract}
Humor plays an essential role in human interactions. However, its precise nature remains elusive. While research in natural language understanding has made significant advancements in recent years, there has been little direct integration of humor research with computational models of language understanding. In this paper, we propose two information-theoretic measures---ambiguity and distinctiveness---derived from a simple model of sentence processing that correspond to key components of humor proposed in the most influential theories. We then test these measures on a set of puns and regular sentences and show that they correlate significantly with human judgments of funniness. Our model is one of the first to integrate general linguistic knowledge and humor theory to model humor computationally. We present it as an example of a framework for applying models of language processing to understand higher-level linguistic and cognitive phenomena.

\end{abstract}

%------------------------------------------------

\keywords{Linguistic humor | Language understanding | Computational modeling} % When adding keywords, separate each term with a straight line: |

%------------------------------------------------

%% Optional for entering abbreviations, separate the abbreviation from
%% its definition with a comma, separate each pair with a semicolon:
%% for example:
%% \abbreviations{SAM, self-assembled monolayer; OTS,
%% octadecyltrichlorosilane}

% \abbreviations{}
\abbreviations{IR, Incongruity Resolution}

%----------------------------------------------------------------------------------------
%	PUBLICATION CONTENT
%----------------------------------------------------------------------------------------

%% The first letter of the article should be drop cap: \dropcap{} e.g.,
%\dropcap{I}n this article we study the evolution of ''almost-sharp'' fronts

\section{Introduction}

\dropcap{L}ove may make the world go round, but humor is the glue that keeps it together. Our everyday experiences serve as evidence that humor plays a critical role in human interactions and composes a significant part of our linguistic, cognitive, and social lives. Previous research has shown that humor is ubiquitous across cultures \cite{martin2010psychology, kruger1996nature}, increases interpersonal attraction \cite{lundy1998heterosexual}, helps resolve intergroup conflicts \cite{smith2000resolving}, and improves psychological wellbeing \cite{martin1993humor}. However, the cognitive basis of humor remains largely a mystery. We understand that humor is important, but what makes us laugh, and why? In this paper, we build upon theories of humor and language processing to computationally model the humor in pun sentences. By providing a formal model of linguistic humor, we aim to shed light on the precise properties of inputs that trigger sensations of mirth.

%As Veale (2004) states, ``Of the few sweeping generalizations one can make about humor that are neither controversial or trivially false, one is surely that humor is a phenomenon that relies on incongruity." 
Many theories of humor have been proposed since the times of Plato and Aristotle (see \cite{attardo1994linguistic} for review). Of these theories, the incongruity-resolution theory has had a strong influence on contemporary studies of humor \cite{suls1983cognitive, attardo2002script, samson2009neural, bartolo2006humor}. Incongruity is often loosely defined as the incompatible and often unexpected interpretations of a stimulus, while resolution involves discovering a ``cognitive rule" that explains the incongruity in a logical manner and thereby reduces it \cite{ritchie1999developing, suls1972two}. The incongruity resolution theory proposes that both the experience of incongruity and its resolution are necessary for humor \cite{suls1983cognitive, ritchie1999developing}. The following riddle may help illustrate the incongruity-resolution model in more concrete terms:
\begin{itemize}
\item[(1)] Q: What is grey, has four legs, and a trunk? \\ A: A mouse on vacation.
\end{itemize}
According to one analysis using the incongruity-resolution model, the riddle is funny because the answer is incongruent with the most natural response to the question (``elephant"). This incongruity is resolved by the realization that ``trunk" is an ambiguous word, and that ``a mouse on vacation" is a valid and logical answer given one meaning of ``trunk." However, the informal nature of this analysis leaves much room for disparate interpretations among humor researchers. The same joke can be analyzed in multiple ways that each support slightly different versions of incongruity resolution. For example, some versions of the incongruity-resolution model claim that at least one of the interpretations generated by the stimulus needs to be inherently inappropriate in some way (cite). Under this view, part of the riddle's humor arises from the fact that a mouse on vacation is inherently a preposterous idea. Instead of analyzing the answer as part of the resolution, this view identifies it as a further source of incongruity, which deemphasizes the importance of resolution for this particular example. While these informal analyses provide important tools for thinking about humor, the lack of computational rigor makes it difficult to operationalize and empirically evaluate the contribution of different factors to the perception of humor \cite{ritchie1999developing}.

More recently, researchers in artificial intelligence and computational linguistics have applied computational tools to examine features of humor. The interest in computational humor is motivated in part by the need for computers to recognize and generate humorous input in order to interact with humans in a more engaging and natural manner \cite{mihalcea2006learning}. However, most work on computational humor focuses either on joke-specific templates and schemata \cite{binsted1996machine, kiddon2011s} or surface features such as innuendo and slang \cite{mihalcea2006learning, semantic2010}. While these approaches are able to identify and produce humorous stimuli within certain constraints, they fall short of testing and building upon a more general cognitive theory of humor.

In this paper, we utilize a simple computational model of sentence processing to derive theory-driven measures of humor. We propose two measures of humor, ambiguity and distinctiveness, each designed to capture aspects of incongruity and resolution. While one could compare multiple formalizations that each capture different interpretations of the informal theory, in this paper we take the first step to propose and test one particular formalization. The ambiguity measure quantifies the degree to which a stimulus has multiple plausible interpretations. The more ambiguous the stimulus, the more opportunity it has to generate incongruous interpretations. For example, the ambiguity of the word ``trunk" allows for multiple interpretations of the question and hence generates incongruous candidate answers. The distinctiveness measure quantifies the degree to which different interpretations are supported by distinct parts of the stimulus. The more distinct the support, the more likely it is that the stimulus can be partitioned into sections that each help explain and resolve the incongruity. For example, the \emph{snout} meaning of ``trunk" is supported by the question part of the riddle, while the \emph{suitcase} meaning of ``trunk" is supported by the answer. Since each interpretation is strongly supported by distinct aspects of the riddle, one can explain and resolve the incongruity experienced by focusing on different parts of the stimulus. We posit that both ambiguity and distinctiveness capture important characteristics of linguistic humor as outlined by the incongruity-resolution model.
%By basing our measures on humor theory, we are able to leverage insights generated by qualitative research.
By formalizing these measures, we can evaluate  more precisely how different cognitive factors contribute to the experience of humor and provide empirical evidence to inform and refine the theory. %Furthermore, by deriving these measures from a model of general sentence processing, we aim to view linguistic humor as a more direct result of language understanding strategies instead of as a separate process driven by surface linguistic cues.

Since the humor of a sentence is tied to its meaning, a formal model of humor requires a formal model of meaning. In order to avoid tackling the largely unsolved problem of formal representations of meaning, we focus on a subset of linguistic humor for which we are able to obtain reasonable approximations for complex sentence meanings. We focus on phonetic puns, which we define as puns containing words that sound identical or similar to other words in English. Since the meanings of a phonetic pun hinge on the meanings of a single phonetically ambiguous word, the space of possible meaning representations is relatively constrained and more easily approximated and formalized. For example, consider the following sentence:
\begin{itemize}
\item[(2)] ``The magician got so mad he pulled his hare out."
\end{itemize}
%
Due to the phonetic ambiguity of ``hare," the phonetic form of this sentence generates two possible interpretations:
\begin{itemize}
\item[(2a)] The magician got so mad he performed the trick of pulling a rabbit out of his hat.
\item[(2b)] The magician got so mad he pulled out the hair on his head.
\end{itemize}

Interpretation (2a) arises if the word ``hare" is interpreted as \emph{hare}; interpretation (2b) arises if the word ``hare" is interpreted as its homophone \emph{hair}. The sentence-level interpretations of a phonetic pun directly correspond to interpretations of a single phonetically ambiguous word. We can thus approximate the meaning of (2a) using the word ``hare" and the meaning of (2b) using the word ``hair." Although this is admittedly a coarse approximation and captures the ``gist" of a sentence rather than its true meaning, it allows us to bypass the challenging and largely unsolved problem of formally representing complex sentence meanings.

%We observe that even though one sees the word ``hare" explicitly when processing (2), the ``hair" interpretation is still highly accessible. 
Given a formal representation of the approximate meanings of sentences, we now introduce a model of how comprehenders arrive at these meanings. Our measures of humor will be derived from a distribution over meanings supported by different aspects of the input. 
Previous research has shown that semantic priming and the sequential structure of language play important roles in early stages of sentence processing. We assume that the distribution over meanings comes from probabilistic integration of uncertain pieces of evidence about the true meaning, which come from the words and their sequential structure. In particular, we assume that both the meanings of the words and the words themselves are uncertain. Consistent with a noisy channel approach, each word may be a correct observation or ÒnoiseÓ. If it is a correct observation, it noisily reflects the sentence meaning. Research on formal models of sentence comprehension suggests that people maintain uncertainty about the surface input when processing a sentence. 
%Comphrehenders assume that communication happens through a ``noisy channel," and that some parts of the input they receive may have been corrupted. 
In order to successfully infer the true meaning of a sentence, comprehenders consider multiple word-level interpretations during processing and rationally incorporate them to arrive at coherent interpretations at the sentence level. 
%By positing noise in the input and modeling comprehension as rational inference under uncertainty, the noisy-channel model is able to explain a variety of phenomena in language processing. 
Here we propose that the humor in phonetic puns may arise from the assumption of a noisy channel. Because the comprehender maintains uncertainty about the input, it is possible for him to arrive at multiple interpretations of a sentence that are each coherent but incongruous with each other. Combining these components of the sentence processing mechanism, we propose a simplified model that incorporates the noisy-channel assumption, the semantic relationship between a sentence's overall meaning and the words that compose it, and the sequential structure of language. Putting these together, via probability, we get a joint distribution over meanings and the ÒmeaningfulÓ subset of words. While our model is not meant to capture deep sentence comprehension, it is driven by psychologically valid components and aims to recover the ``gist" of sentence meanings. We believe this is sufficiently powerful as a first step for modeling the humor that may arise from processing phonetically ambiguous sentences.

\subsection{Model}

Suppose a sentence is composed of a vector of words $\vec w = \{w_1, \dots, w_i, h, w_{i+1}, \dots ,w_n\}$, where $h$ is a phonetically ambiguous word. We construct a simple generative model for $\vec w$ (see Figure~\ref{generativeModel}) that captures the relationship between the meaning gist of a sentence and the words that compose it. Given a latent sentence meaning gist $m$, each word is generated independently by first deciding if it explicitly reflects the sentence meaning or is corrupted by noise. This process is determined by an indicator variable vector $\vec f$, where $w_i$ is a meaningful word if $f_i = 1$ and a noise word if $f_i = 0$. When $w_i$ is a meaningful word, it is sampled in proportion to its semantic relevance to $m$. When it is a corrupted word, we assume that its corruption is affected by the immediately preceding words and sample it from an n-gram language model.

To select the sentence meanings $m$, we exploit the convenient property of phonetic puns described before. We introduce the simplifying assumption that the sentence meanings $m$ correspond to plausible interpretations of the homophone word $h$, which are constrained by phonetic similarity. For example, ``hare" is a phonetically ambiguous word, and the homophones \emph{hare} and \emph{hair} can approximate the two possible sentence meanings $m_1$ and $m_2$. While this approximation is admittedly coarse, it makes use of the reasonable assumption that sentences containing the word ``hare" will generally be about the topic \emph{hare}, and sentences that have the word ``hair" will generally be about the topic \emph{hair}. This assumption reduces the ill-defined space of sentence meanings to the simple proxy of alternate spellings for phonetically ambiguous words.

\begin{figure}
\centering
\tikzset{decorate sep/.style 2 args=
{decorate,decoration={shape backgrounds,shape=circle,shape size=#1,shape sep=#2}}}
\begin{tikzpicture}
\tikzstyle{place}=[circle,draw,inner sep=2pt,minimum size=0.95cm]
 \tikzstyle{plate}=[rectangle,draw,inner sep=0pt]
 \node[place] (m) at (0,3) {$m$};
 \node[place] (w1) at (-2,1) {$w_1$};
 \node[place] (w2) at (-1,1) {$w_2$};
 \node[place] (h) at (0.5,1) {$h$}; 
 \node[place] (wn) at (2,1) {$w_n$};
 \node[place] (f1) at (-2, -0.5) {$f_1$};
 \node[place] (f2) at (-1, -0.5) {$f_2$};
\node[place] (fh) at (0.5, -0.5) {$f_h$};
\node[place] (fn) at (2, -0.5) {$f_n$};
 %\node[place] (wordsprior) at (0,4.5) {$\wordsprior$};
\draw [->] (m) -- (w1);
\draw [->] (m) -- (w2);
\draw [->] (m) -- (h);
\draw [->] (m) -- (wn);
\draw [->] (f1) -- (w1);
\draw [->] (f2) -- (w2);
\draw [->] (fh) -- (h);
\draw [->] (fn) -- (wn);
\draw[decorate sep={0.3mm}{1.65mm},fill] (-0.41,1) -- (-0.05,1);
\draw[decorate sep={0.3mm}{1.65mm},fill] (-0.41,-0.5) -- (-0.05,-0.5);
\draw[decorate sep={0.3mm}{1.65mm},fill] (1.09,1) -- (1.45,1);
\draw[decorate sep={0.3mm}{1.65mm},fill] (1.09,-0.5) -- (1.45,-0.5);
\end{tikzpicture}
\caption{Generative model of a sentence. Each word $w_i$ is generated based on the sentence gist$m$ if the indicator variable $f_i$ puts it in semantic focus; otherwise it is generated as noise from a trigram distribution given the previous two words. }
\label{generativeModel}
\end{figure}

Using the generative model described, we can infer the joint probability distribution $P(m, \vec f | \vec w) $ of the sentence topic $m$ and the indicator variables $\vec f$ given a set of words $\vec w$. This distribution can be factorized as the following:
\begin{align}
P(m, \vec f | \vec w) = P(m | \vec w) P(\vec f | m, \vec w) 
\end{align}
We derive two formal components of humor from the two terms on the righthand side, which we call ambiguity and distinctiveness. Ambiguity captures the degree to which sentence meanings are similarly likely. This can be quantified as a summary of the binomial distribution $P(m | \vec w)$. If the entropy of this distribution is low, then one meaning is much more likely than the other. If the entropy is high, then both meanings are similarly likely. 

Distinctiveness measures the degree to which two sentence meanings are supported by distinct parts of the sentence. This can be quantified as a summary of the distribution $P(\vec f | m, \vec w)$. Given $\vec w$ and the topic $m_1$, which is directly observed in the sentence, we compute the distribution $F_1 = P(\vec f | m_1, \vec w)$. Given $\vec w$ and the topic $m_2$, we compute the distribution $F_2 = P(\vec f | m_2, \vec w)$. We wish to measure how different the distribution over topic words would be given different sentence meanings. We use a Kullback-Leibler divergence score $D_{KL}(F_1 || F_2)$ to measure the distance between $F_1$ and $F_2$. A low KL score indicates that the possible sentence meanings are supported by similar subsets of the sentence. A high KL score indicates that the sentence meanings are each strongly supported by distinct subsets of the sentence. 
%Note that this measure is not symmetric between $F_1$ and $F_2$. Since $m_1$ is the word that is explicitly observed in the sentence (e.g. \emph{hare}), the relationship between the two distributions is inherently asymmetric. 
Together, our ambiguity and distinctiveness measures constitute a two-dimensional formalization of humor. 

The two formal measures we derived have natural connections to the incongruity-resolution model of humor. Under the assumption that the two sentence meanings are sufficiently different from each other, a high ambiguity score suggests the coexistence of two incompatible interpretations, which is a characterization of ``incongruity." A high distinctiveness score suggests that given one sentence meaning, the comprehender should regard one set of words as meaningful words, while given another sentence meaning, attention should be directed to a different set of words. This allows the comprehender to pay attention to different sets of words given different candidate sentence meanings, effectively ``resolving" the incongruity by discovering the cognitive rule that partitions the sentence into distinct parts that are each internally harmonious.
%------------------------------------------------

\section{Results}
We evaluated the ambiguity and distinctiveness measures on a set of $435$ phonetically ambiguous sentences. Of these sentences, $65$ are identical homophone puns and $80$ are puns where the two candidate meanings sound similar but are not identical to each other (near homophone puns). The remaining $290$ sentences are non-pun control sentences that contain the same phonetically ambiguous words as the puns. Table \ref{sentences} shows an example of each type of sentence.

\begin{table}[h]
\caption{Example sentences}
\label{sentences}
\begin{tabular}{l l l}
\hline
\textbf{Type} & \textbf{Hom} & \textbf{Example}\\
\hline
Pun & Identical & The magician was so mad he pulled his hare out. \\
Pun & Near & A dentist has to tell a patient the whole tooth. \\
Non-pun & Identical & The hare ran rapidly across the field. \\
Non-pun & Identical & Some people have lots of hair on their heads. \\
Non-pun & Near & A dentist examines one tooth at a time. \\
Non-pun & Near & She always speaks the truth. \\
\hline
\end{tabular}
\end{table}

We obtained human ratings of funniness for each of the $435$ sentences (see Materials and Methods). Applying the derivations and relatedness measures described in the Methods section, we computed an ambiguity and distinctiveness score for each sentence. As predicted, ambiguity was significantly higher for pun sentences than non-pun sentences ($F(1, 433) = 108.4, p < 0.0001$). This suggests that our ambiguity measure successfully captures characteristics that distinguish puns from other phonetically ambiguous sentences. Intuitively, while non-pun sentences also contain phonetically ambiguous words, their interpretations are less ambiguous because the words are highly semantically related to only one sentence meaning. Our measure of distinctiveness differs marginally significantly across sentence types ($F(1, 433) = 47.1, p < 0.1)$ Using both ambiguity and distinctiveness as dimensions that formalize humor, we can distinguish among pun and non-pun sentences, as shown in Figure~\ref{ellipse}. Figure~\ref{ellipse} shows the standard error ellipses for the two sentence types in the two-dimensional space of ambiguity and distinctiveness. Although there is a fair amount of noise in the predictors (likely due to simplifying assumptions, the need to use empirical measures of relatedness, and the inherent complexity of humor), we see that pun sentences tend to cluster at a space with higher ambiguity and distinctiveness, while non-pun sentences score significantly lower on both measures. A linear regression model showed that both ambiguity and distinctiveness are significant predictors of funniness ratings across all $435$ sentences. Together, the two predictors capture a modest but significant amount of the reliable variance in funniness ratings ($F(2,432) = 76.79,  r = 0.51, p < 0.0001$; see Table~\ref{coefficients}). 

\begin{figure}[t]
\centerline{\includegraphics[width=8.6cm]{ellipse.pdf}}
\caption{Standard error ellipses of ambiguity and distinctiveness across sentence types. Puns score higher on ambiguity and distinctiveness; non-puns have low ambiguity and distinctiveness.}\label{ellipse}
\end{figure}

\begin{figure}[ht]
\centerline{\includegraphics[width=8.6cm]{scatter.pdf}}
\caption{Figure caption}\label{placeholder}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{l l l l}\hline
& {Estimate} & Std. Error & {p value} \\\hline
Intercept & $-0.830$ & $0.1070$ & $< 0.0001$ \\
Ambiguity & $1.899$ & $0.212$ & $< 0.0001$\\
Distinctiveness & $0.568$ & $0.082$ & $< 0.0001$\\\hline
\end{tabular}
\caption{Regression coefficients using ambiguity and distinctiveness to predict funniness ratings}
\label{coefficients}
\end{table}

We now examine the measures' quantitative predictions of funniness within puns. Ambiguity does not correlate with human ratings of funniness within the $145$ pun sentences ($r = 0.03, p > 0.05$), suggesting that it alone is unable to distinguish highly funny puns from mediocre puns. On the other hand, distinctiveness ratings correlate significantly with human ratings of funniness within pun sentences ($r = 0.28, p < 0.001$). This suggests that while ambiguity distinguishes puns from non-puns, distinctiveness is needed to separate very funny puns from mediocre ones.

Besides predicting the funniness of a sentence, our model can also tell us which particular features of a pun make it amusing. For each sentence, we identified the set of words that is most likely to be meaningful and in focus given each latent sentence meaning $m$ and $\vec w$. In other words, we identified
$
\arg\max_{\vec f} P(\vec f | m, \vec w)
$.
 Table~\ref{focusExamples} shows two groups of sentences. Sentences in each group contain the same pair of candidate meanings for the target word $h$; however, they differ on ambiguity, distinctiveness, and funniness.  Words that are most likely to be in focus given $m_1$ are in red; words that are most likely given $m_2$ are in green; and words that are most likely given both meanings are in dark blue. We observe that the two pun sentences (which are significantly funnier) have more distinctive and balanced sets of meaningful words for each sentence meaning than other sentences in their groups. Non-pun sentences tend to have no words in support of the meaning that was not observed. Furthermore, the colorful words in each pun sentence---for example, the fact that magicians tend to perform magic tricks with hares, and people tend to be described as pulling out their hair when angry---are what one might intuitively use to explain why the sentence is a pun. Our model thus not only uses ambiguity and distinctiveness to predict when a sentence is a pun, but also provides a natural way of explaining which aspects of a pun make it funny. 

%------------------------------------------------

\section{Discussion}
In this paper we presented a simple model of sentence processing and derived formal measures to predict human judgments of humor for a set of sentences. We showed that a noisy-channel model allows for flexible context selection that may give rise to sophisticated linguistic meaning such as humor during sentence processing. In particular, the comprehender's consideration of whether a word is semantically meaningful allows her to switch between sentence meanings and entertain the possibility that the speaker intends both meanings at once. As writer and philosopher Henri Bergson wrote, ``A pun is a sentence or utterance in which two ideas are expressed, and we are confronted with only one series of words." The noisy-channel model allows us to formally capture this duality of meaning and to show how it may be manifested by focusing on different sets of words.

Our analysis also contributes to humor theory by providing evidence that the formalizations of ambiguity and distinctiveness we proposed may account for separate aspects of humor appreciation. Ambiguity serves to distinguish humorous input from normal sentences, while distinctiveness predicts the fine-grained degree of funniness within humorous input. We speculate that the degree of funniness captured by distinctiveness may be related to the recognition of a cognitive rule that meaningfully separates one group of words from another. While the connections between our measures and the incongruity-resolution model of humor are tenuous, these results seem to support the idea that incongruity is a necessary condition for humor, while the intensity of humor experienced may depend upon incongruity resolution.

%We described a probabilistic model of sentence comprehension that uses noisy channel assumptions to compute the meaning of a phonetically ambiguous word. 
Although our task in this paper is limited in scope, it is a step towards developing models that can explain rich linguistic phenomena such as humor. Future work may incorporate more sophisticated models of language understanding---ones that consider deeper semantic representations and multi-sentence discourse---to derive corresponding measures for different types of jokes. Besides seeking to understand linguistic humor for its own sake, these higher-order phenomena can serve as probes for developing models of language processing that account for a wider range of linguistic behavior, and may even shed light on how language understanding works more generally. We believe that our work contributes to research in humor theory, computational humor, and language understanding, such that some day we can build robots that make us laugh and understand the appreciation for humor that makes us uniquely human.

\begin{table*}[t]
\centering
\hfill{}
\begin{tabular}{l l l l l l l}
\hline
\textbf{$m1$}& \textbf{$m2$} & \textbf{Type} & \textbf{Sentence and Semantic Focus Sets}& \textbf{Amb.} & \textbf{Dist.} & \textbf{Funniness}\\\hline
\multirow{4}{*}{{\color{Red} hare}} & \multirow{4}{*}{{\color{Emerald} hair}} & Pun & The \textbf{{\color{Red}magician}} got so mad he \textbf{{\color{Emerald}pulled}} his \textbf{{\color{Red}hare}} out. & $0.15$ & $1.36$ & $1.714$\\ 
&&Non-pun & The \textbf{{\color{Red} hare}} \textbf{{\color{Red} ran}} \textbf{{\color{Red} rapidly}} through the \textbf{{\color{Red} fields}}. & $1.43E^{-5}$ &	$1.07$ &	$-0.400$\\
&&Non-pun & Most \textbf{{\color{Emerald} people}} have \textbf{{\color{Emerald} lots}} of \textbf{{\color{Emerald} hair}} on their \textbf{{\color{Emerald} heads}}. & $9.47E^{-11}$ &	$1.55$ & $-0.343$ \\\hline

\multirow{4}{*}{{\color{Red} tooth}} & \multirow{4}{*}{{\color{Emerald} truth}} & Pun & A \textbf{{\color{Red} dentist}} has to \textbf{{\color{Emerald} tell}} the \textbf{{\color{Red} patient}} the \textbf{{\color{Emerald} whole}} \textbf{{\color{Red} tooth}}. & $0.10$ & $1.64$ & $1.41$\\

&&Non-pun & A \textbf{{\color{Red} dentist}} \textbf{{\color{NavyBlue} examines}} one \textbf{{\color{Red}tooth}} at a time. & $8.92E^{-5}$ & 	$1.23$ &	$-0.45$\\

&&Non-pun & She always \textbf{{\color{Emerald} speaks}} the \textbf{{\color{Emerald} truth}}. & $3.85E^{-10}$	& $0.72$ &	$-0.46$ \\
\hline
\end{tabular}
\hfill{}
\caption{Sets of semantically meaningful words, ambiguity/distinctiveness scores, and funniness ratings for two groups of sentences. Words in red are semantically meaningful given $m_1$; green given $m_2$; blue given both.}
%Semantic focus sets for all sentences can be found at \url{http://www.stanford.edu/~justinek/Pun/focusSets.html}
%
\label{focusExamples}
\end{table*}
%----------------------------------------------------------------------------------------
%	MATERIALS AND METHODS
%----------------------------------------------------------------------------------------

%% Optional Materials and Methods Section
%% The Materials and Methods section header will be added automatically.

\begin{materials}
\section{Model details}
For simplification, our model disregards function words in the sentences and treats them as only indirectly contributing to the overall meaning of a sentence. Each of the $w_i$ in $\vec w$ in this paper is a content word. Here we will describe the derivations for ambiguity and distinctiveness in more detail.

\textbf{Ambiguity:} $P(m | \vec w)$ is a Bernoulli distribution over the two meaning values $m_1$ and $m_2$ given the observed words. If the entropy of this distribution is low, this means that the probability mass is concentrated on only one meaning, and the other meaning is unlikely given the observed words. If entropy is high, this means that the probability mass is more evenly distributed among $m_1$ and $m_1$, and the two interpretations are similarly likely given the sentence. The entropy of $P(m | \vec w)$ is thus a natural measure of the degree of ambiguity present in a sentence. We compute $P(m | \vec w)$ as follows:
\begin{align}
P(m | \vec w) &= \sum_{\vec f} P(m, \vec f | \vec w) \\
&\propto \sum_{\vec f} P(\vec w | m, \vec f) P(m) P(\vec f) \\
&= \sum_{\vec f} \bigg (P(m)P(\vec f)\prod_i P(w_i | m, f_i) \bigg)
\end{align}
%From Bayes' theorem, this is proportional to the following:
%$$
%\sum_{\vec f} P(\vec w | m, \vec f) P(m) P(\vec f)= \sum_{\vec f} \bigg (P(m)P(\vec f)\prod_i P(w_i | m, f_i) \bigg)
%$$
We approximate $P(m)$ as the unigram frequency of the words that represent $m$. For example, $P(m=hare)$ is approximated as $P(m=\text{``hare"})$. We also assume a uniform prior probability over all subsets of the words being semantically meaningful. In other words, $P(f_i = 1) = 0.5$, and so $P(\vec f)$ is a constant.  As for $P(w_i | m, f_i)$, the probability depends upon the value of the indicator variable $f_i$. If $f_i = 1$, $w_i$ is semantically meaningful 
and was sampled in proportion to its relatedness with the sentence meaning $m$. If $f_i = 0$, then $w_i$ was generated from a noise process and sampled in proportion to its probability given the previous two words (including function words). In other words, semantically meaningful words are generated based on the meaning, while noise words are generated based on the sequential structure of language. From the generative model, 
\[
    P(w_i | m, f_i) = 
\begin{cases}
    P(w_i | m), &\text{if } f=1\\
    P(w_i | bigram_i),& \text{if } f=0\\
\end{cases}
\]
where $P(w_i |m)$ is estimated using measures described in Experiment 2. $P(w_i | bigram_i)$ was estimated from the Google Ngrams corpus, where $bigram_i$ is the bigram that immediately precedes $w_i$. Once we derive $M=P(m | \vec w)$, we compute its information-theoretic entropy as a measure of ambiguity:
$$
Amb(M) = -\sum_{i} {P(m_i | \vec w) \log P(m_i | \vec w)}
$$

\textbf{Distinctiveness:} We next turn to the distribution over indicator variables $\vec f$ given a sentence meaning. Given Bayes' Rule, $P(\vec f | m, \vec w)$ is computed as follows:
\begin{align}
P(\vec f | m, \vec w) \propto P(\vec w | m, \vec f) P(\vec f | m)
\end{align}
Since $\vec f$ and $m$ are independent, $P(\vec f | m) = P(\vec f)$. 
Let $F_1$ denote the distribution $P(\vec f | m_1, \vec w)$ and $F_2$ denote the distribution $P(\vec f | m_2, \vec w)$. $F_1$ and $F_2$ represent the distributions over semantically meaningful sets assuming the sentence topic $m_1$ and $m_2$, respectively. We use the Kullback-Leibler divergence score $D_{KL}(F_1 || F_2)$ to measure the distance between $F_1$ and $F_2$. This score measures how ``distinct" the semantically meaningful sets are given $m_1$ and $m_2$. A low KL score would indicate that meanings $m_1$ and $m_2$ are supported by similar subsets of the sentence; a high KL score would indicate that $m_1$ and $m_2$ are supported by distinct subsets of the sentence. Based on the definition of KL,
$$
Dist(F_1 || F_2) =  \sum_i \ln\left(\frac{F_1(i)}{F_2(i)}\right) F_1(i).\!
$$

\section{Experiment 1}
We collected pun sentences from a website called ``Pun of the Day" (\url{http://www.punoftheday.com/}), which contains over a thousand puns submitted by users. Puns were selected such that the ambiguous item in each pun is a single phonetically ambiguous word. We obtained $40$ puns where the phonetically ambiguous word has identical homophones (identical-homophone puns). To obtain more identical homophone pun items, a research assistant generated an additional $25$ pun sentences based on a separate list of homophone words. We then selected $80$ puns where the phonetically ambiguous word has a similar-sounding word that is not an identical homophone (near-homophone puns). This resulted in a total of $145$ pun sentences. We selected $290$ corresponding non-pun sentences from an online version of Heinle's Newbury House Dictionary of American English (\url{http://nhd.heinle.com/}). We chose sample sentences included in the definition of the homophone word. $145$ of the sentences contain the ambiguous words from the pun sentences, and $145$ of them contain the alternative homophones. This design ensured that puns and non-pun sentences contain the same set of phonetically ambiguous words. Table~\ref{sentences} shows example sentences from each category.

% fill in for near homophone puns
We obtained funniness ratings for each of the sentences. The$195$ identical homophone sentences were rated by $93$ subjects on Amazon's Mechanical Turk. Each subject read roughly $60$ sentences in random order, counterbalanced for the sentence types, and rated each sentence on funniness and correctness. The average split-half correlation of funniness ratings was $0.83$. Pun sentences were rated as significantly funnier than non-pun sentences ($F(2, 232) = 415.3, p < 0.0001$). The $240$ near homophone sentences were rated by $158$ subjects on Mechanical Turk. Each subject read $40$ sentences in random order, counterbalanced for the sentence types, and rated each sentence on funniness. The average split-half correlation of funniness ratings was $X$. We z-scored the ratings and used the average z-scored ratings across subjects as human judgments of funniness.

\section{Experiment 2} As described in the model details, computing ambiguity and distinctiveness requires the prior probabilities of meanings $P(m)$ (approximated as the unigram probabilities of the words that denote the meanings), the probabilities of words given the preceding bigram $P(w | bigram)$, and the conditional probabilities of each word in the sentence given a meaning $P(w | m)$. While we computed $P(m)$ and $P(w | bigram)$ directly from the Google Web unigram corpus, $P(w | m)$ is difficult to obtain through traditional topic models trained on corpora due to data sparsity. We thus measure it empirically with an experiment.

We approximate $P(w | m)$ using an empirical measure of the semantic relatedness between $w$ and $m$, denoted $R(w, m)$. We use $R(w, m)$ as a proxy for point wise mutual information between $w$ and $m$, defined as follows:
\begin{align}
R(w, m)= {\log \frac{P(w, m)}{P(w)P(m)}} = \log P(w | m) - \log P(w)
\end{align}
We assume that human ratings of relatedness between two words $R'(w, m)$ approximate true relatedness up to an additive constant $z$ and assume $z=0$ for our purposes. With the proper substitutions and transformations, 
\begin{align}
P(w | m) = e^{R'(w, m) + z} P(w)
\end{align}

To obtain $R'(w, m)$ for each of the words $w$ in the stimuli sentences, we recruited $200$ subjects on Amazon's Mechanical Turk to rate word pairs on their semantic relatedness. Since it is difficult to obtain the relatedness rating of a word with itself, we used a free parameter $r$ and fit it to data ($r=13$). Function words were removed from each of the sentences in our dataset, and the remaining words were paired with each of the interpretations of the homophone sequence (e.g., for the pun in Table~\ref{identical-sentences}, ``magician" and ``hare" is a legitimate word pair, as well as ``magician" and ``hair"). This resulted in $1460$ distinct word pairs. Each subject saw $146$ pairs of words in random order and were asked to rate how related each word pair is using a scale from $1$ to $10$. The average split-half correlation of the relatedness ratings was $0.916$, indicating that semantic relatedness was a reliable measure. We used the average z-scored relatedness measure for each word pair to obtain $R(w, m)'$ and Google Web unigrams to obtain $P(w)$, thus computing $P(w | m)$ for all word and meaning pairs. This completed the values needed to compute ambiguity and distinctiveness for all sentences.


\end{materials}

%----------------------------------------------------------------------------------------
%	APPENDICES (OPTIONAL)
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

\begin{acknowledgments}
This work was partially supported by a grant from the Spanish Ministry of Science and Technology.
\end{acknowledgments}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%% PNAS does not support submission of supporting .tex files such as BibTeX.
%% Instead all references must be included in the article .tex document. 
%% If you currently use BibTeX, your bibliography is formed because the 
%% command \verb+\bibliography{}+ brings the <filename>.bbl file into your
%% .tex document. To conform to PNAS requirements, copy the reference listings
%% from your .bbl file and add them to the article .tex file, using the
%% bibliography environment described above.  

%%  Contact pnas@nas.edu if you need assistance with your
%%  bibliography.

% Sample bibliography item in PNAS format:
%% \bibitem{in-text reference} comma-separated author names up to 5,
%% for more than 5 authors use first author last name et al. (year published)
%% article title  {\it Journal Name} volume #: start page-end page.
%% ie,
% \bibitem{Neuhaus} Neuhaus J-M, Sitcher L, Meins F, Jr, Boller T (1991) 
% A short C-terminal sequence is necessary and sufficient for the
% targeting of chitinases to the plant vacuole. 
% {\it Proc Natl Acad Sci USA} 88:10362-10366.


%% Enter the largest bibliography number in the facing curly brackets
%% following \begin{thebibliography}

\begin{thebibliography}{10}
%
%\bibitem{lundy1998heterosexual,
%  title={Heterosexual romantic preferences: The importance of humor and physical Äness for different types of relationships},
%  author={Lundy, Duane E and Tan, Josephine and Cunningham, Michael R},
%  journal={Personal Relationships},
%  volume={5},
%  number={3},
%  pages={311--325},
%  year={1998},
%  publisher={Wiley Online Library}
%}
%
%\bibitem{smith2000resolving,
%  title={Resolving conflict with humor in a diversity context},
%  author={Smith, Wanda J and Harrington, K Vernard and Neck, Christopher P},
%  journal={Journal of Managerial Psychology},
%  volume={15},
%  number={6},
%  pages={606--625},
%  year={2000},
%  publisher={MCB UP Ltd}
%}
%
%\bibitem{kruger1996nature,
%  title={The nature of humor in human nature: Cross-cultural commonalities},
%  author={Kruger, Arnold},
%  journal={Counselling Psychology Quarterly},
%  volume={9},
%  number={3},
%  pages={235--241},
%  year={1996},
%  publisher={Taylor}
%}
%
%
%\bibitem{mobbs2003humor,
%  title={Humor modulates the mesolimbic reward centers},
%  author={Mobbs, Dean and Greicius, Michael D and Abdel-Azim, Eiman and Menon, Vinod and Reiss, Allan L},
%  journal={Neuron},
%  volume={40},
%  number={5},
%  pages={1041--1048},
%  year={2003},
%  publisher={Elsevier}
%}
%
%\bibitem{martin2010psychology,
%  title={The psychology of humor: An integrative approach},
%  author={Martin, Rod A},
%  year={2010},
%  publisher={Access Online via Elsevier}
%}
%
%\bibitem{martin1993humor,
%  title={Humor, coping with stress, self-concept, and psychological well-being},
%  author={Martin, Rod A and Kuiper, Nicholas A and Olinger, L and Dance, Kathryn A},
%  year={1993}
%}
%
%\bibitem{ZhaZha}
%Z.~Zhang and H.~Zha, {\em Principal manifolds and nonlinear dimension
%  reduction via local tangent space alignement}, Tech. Report CSE-02-019,
%  Department of computer science and engineering, Pennsylvania State
%  University, 2002.
%

\bibitem{martin2010psychology}
Rod~A Martin.
\newblock The psychology of humor: An integrative approach.
\newblock 2010.

\bibitem{kruger1996nature}
Arnold Kruger.
\newblock The nature of humor in human nature: Cross-cultural commonalities.
\newblock {\em Counselling Psychology Quarterly}, 9(3):235--241, 1996.

\bibitem{lundy1998heterosexual}
Duane~E Lundy, Josephine Tan, and Michael~R Cunningham.
\newblock Heterosexual romantic preferences: The importance of humor and
  physical Äness for different types of relationships.
\newblock {\em Personal Relationships}, 5(3):311--325, 1998.


\bibitem{martin1993humor}
Rod~A Martin, Nicholas~A Kuiper, L~Olinger, and Kathryn~A Dance.
\newblock Humor, coping with stress, self-concept, and psychological
  well-being.
\newblock 1993.

\bibitem{smith2000resolving}
Wanda~J Smith, K~Vernard Harrington, and Christopher~P Neck.
\newblock Resolving conflict with humor in a diversity context.
\newblock {\em Journal of Managerial Psychology}, 15(6):606--625, 2000.

@book{attardo1994linguistic,
  title={Linguistic theories of humor},
  author={Attardo, Salvatore},
  volume={1},
  year={1994},
  publisher={Walter de Gruyter}
}

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{article}

%----------------------------------------------------------------------------------------
%	FIGURES AND TABLES
%----------------------------------------------------------------------------------------

%% Adding Figure and Table References
%% Be sure to add figures and tables after \end{article}
%% and before \end{document}

%% For figures, put the caption below the illustration.
%%
%% \begin{figure}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure}

%% For Tables, put caption above table
%%
%% Table caption should start with a capital letter, continue with lower case
%% and not have a period at the end
%% Using @{\vrule height ?? depth ?? width0pt} in the tabular preamble will
%% keep that much space between every line in the table.

%% \begin{table}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
%% table text
%% \end{tabular}
%% \end{table}

%% For two column figures and tables, use the following:

%% \begin{figure*}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure*}

%% \begin{table*}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{ccc}
%% table text
%% \end{tabular}
%% \end{table*}

%----------------------------------------------------------------------------------------
%\bibliographystyle{apa}
%\bibliography{bib}

\end{document}