%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proceedings of the National Academy of Sciences (PNAS)
% LaTeX Template
% Version 1.0 (19/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% The PNAStwo class was created and is owned by PNAS:
% http://www.pnas.org/site/authors/LaTex.xhtml
% This template has been modified from the blank PNAS template to include
% examples of how to insert content and drastically change commenting. The
% structural integrity is maintained as in the original blank template.
%
% Original header:
%% PNAStmpl.tex
%% Template file to use for PNAS articles prepared in LaTeX
%% Version: Apr 14, 2008
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

%------------------------------------------------
% BASIC CLASS FILE
%------------------------------------------------

%% PNAStwo for two column articles is called by default.
%% Uncomment PNASone for single column articles. One column class
%% and style files are available upon request from pnas@nas.edu.

%\documentclass{pnasone}
\documentclass{pnastwo}

%------------------------------------------------
% POSITION OF TEXT
%------------------------------------------------

%% Changing position of text on physical page:
%% Since not all printers position
%% the printed page in the same place on the physical page,
%% you can change the position yourself here, if you need to:

% \advance\voffset -.5in % Minus dimension will raise the printed page on the 
                         %  physical page; positive dimension will lower it.

%% You may set the dimension to the size that you need.

%------------------------------------------------
% GRAPHICS STYLE FILE
%------------------------------------------------

%% Requires graphics style file (graphicx.sty), used for inserting
%% .eps/image files into LaTeX articles.
%% Note that inclusion of .eps files is for your reference only;
%% when submitting to PNAS please submit figures separately.

%% Type into the square brackets the name of the driver program 
%% that you are using. If you don't know, try dvips, which is the
%% most common PC driver, or textures for the Mac. These are the options:

% [dvips], [xdvi], [dvipdf], [dvipdfm], [dvipdfmx], [pdftex], [dvipsone],
% [dviwindo], [emtex], [dviwin], [pctexps], [pctexwin], [pctexhp], [pctex32],
% [truetex], [tcidvi], [vtex], [oztex], [textures], [xetex]


%------------------------------------------------
% OPTIONAL POSTSCRIPT FONT FILES
%------------------------------------------------

%% PostScript font files: You may need to edit the PNASoneF.sty
%% or PNAStwoF.sty file to make the font names match those on your system. 
%% Alternatively, you can leave the font style file commands commented out
%% and typeset your article using the default Computer Modern 
%% fonts (recommended). If accepted, your article will be typeset
%% at PNAS using PostScript fonts.

% Choose PNASoneF for one column; PNAStwoF for two column:
%\usepackage{PNASoneF}
%\usepackage{PNAStwoF}

%------------------------------------------------
% ADDITIONAL OPTIONAL STYLE FILES
%------------------------------------------------

%% The AMS math files are commonly used to gain access to useful features
%% like extended math fonts and math commands.
\usepackage{multirow}
%\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{etex}
\usepackage{color}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{decorations.shapes}
\usepackage{amssymb,amsfonts,amsmath}

%------------------------------------------------
% OPTIONAL MACRO FILES
%------------------------------------------------

%% Insert self-defined macros here.
%% \newcommand definitions are recommended; \def definitions are supported

%\newcommand{\mfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
%\def\s{\sigma}

%------------------------------------------------
% DO NOT EDIT THIS SECTION
%------------------------------------------------

%% For PNAS Only:
\contributor{Submitted to Proceedings of the National Academy of Sciences of the United States of America}
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHORS
%----------------------------------------------------------------------------------------

\title{A computational model of linguistic humor} % For titles, only capitalize the first letter

%------------------------------------------------

%% Enter authors via the \author command.  
%% Use \affil to define affiliations.
%% (Leave no spaces between author name and \affil command)

%% Note that the \thanks{} command has been disabled in favor of
%% a generic, reserved space for PNAS publication footnotes.

%% \author{<author name>
%% \affil{<number>}{<Institution>}} One number for each institution.
%% The same number should be used for authors that
%% are affiliated with the same institution, after the first time
%% only the number is needed, ie, \affil{number}{text}, \affil{number}{}
%% Then, before last author ...
%% \and
%% \author{<author name>
%% \affil{<number>}{}}

%% For example, assuming Garcia and Sonnery are both affiliated with
%% Universidad de Murcia:
%% \author{Roberta Graff\affil{1}{University of Cambridge, Cambridge,
%% United Kingdom},
%% Javier de Ruiz Garcia\affil{2}{Universidad de Murcia, Bioquimica y Biologia
%% Molecular, Murcia, Spain}, \and Franklin Sonnery\affil{2}{}}

\author{Justine T. Kao\affil{1}{Stanford University},
Roger Levy\affil{2}{University of California, San Diego}
\and
Noah D. Goodman\affil{1}{}}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

%----------------------------------------------------------------------------------------

\maketitle % The \maketitle command is necessary to build the title page

\begin{article}

%----------------------------------------------------------------------------------------
%	ABSTRACT, KEYWORDS AND ABBREVIATIONS
%----------------------------------------------------------------------------------------

\begin{abstract}
Humor plays an essential role in human interactions. However, its precise nature remains elusive. While research in natural language understanding has made significant advancements in recent years, there has been little direct integration of humor research with computational models of general language understanding. In this paper, we propose two information-theoretic measures of humor--ambiguity and distinctiveness--inspired by humor theories and directly derived from a simple model of sentence processing. We then test these measures on a set of puns and non-pun sentences and show that they correlate significantly with human judgments of funniness. Our model is one of the first to integrate general linguistic knowledge, speaker intent, and humor theory to model humor computationally. We present it as a framework for applying models of language processing to understand higher-level linguistic and cognitive phenomena.

\end{abstract}

%------------------------------------------------

\keywords{Linguistic humor | Language understanding | Computational modeling} % When adding keywords, separate each term with a straight line: |

%------------------------------------------------

%% Optional for entering abbreviations, separate the abbreviation from
%% its definition with a comma, separate each pair with a semicolon:
%% for example:
%% \abbreviations{SAM, self-assembled monolayer; OTS,
%% octadecyltrichlorosilane}

% \abbreviations{}
\abbreviations{IR, Incongruity Resolution}

%----------------------------------------------------------------------------------------
%	PUBLICATION CONTENT
%----------------------------------------------------------------------------------------

%% The first letter of the article should be drop cap: \dropcap{} e.g.,
%\dropcap{I}n this article we study the evolution of ''almost-sharp'' fronts

\section{Introduction}

\dropcap{I}magine living a day without humor. From friendly exchanges with an affable stranger to sidesplitting laughter among close friends, our everyday experiences clearly show that humor plays an essential role in human interactions. Many studies suggest that humans are naturally attracted to and can often significantly benefit from humorous stimuli [3-8]. Here we develop a computational model to examine this ubiquitous and fundamental phenomenon, with the aim of shedding light on the conditions in which the mind perceives humor in language.

Researchers in artificial intelligence have argued that given the importance of humor in human communication, computers need to generate and detect humor in order to interact with humans more effectively \shortcite{mihalcea2006learning}. However, most work in computational humor has focused either on joke-specific templates and schemata \shortcite{binsted1996machine, kiddon2011s} or surface linguistic features that predict humorous intent \shortcite{mihalcea2006learning, semantic2010}. The former type of studies is restricted to identifying jokes with a very specific format and structure, and the latter type falls short of testing or building upon more general theories of humor.Our work moves beyond these approaches and directly utilizes a model of sentence comprehension to derive theory-driven measures of humor. Incongruity, defined as incompatible and often schema-violating interpretations of a stimulus, has received the most attention as a compelling requisite of humor [cite]. As Veale (2004) states, “Of the few sweeping generalizations one can make about humor that are neither controversial or trivially false, one is surely that humor is a phenomenon that relies on incongruity.” However, other scholars argue that incongruity alone is insufficient, since incongruous situations may simply appear senseless or dissonant instead of humorous [cite]. These scholars propose a two-stage model of humor termed the incongruity-resolution (IR) model, in which resolution is commonly understood as discovery of a cognitive rule that reconciles the incongruous parts of a situation [cite]. While informal models of humor have provided a useful framework for analyzing necessary and sufficient conditions of humor, the lack of computational rigor makes it difficult to operationalize and empirically evaluate the role of different cognitive factors in the perception of humor. Within the IR model, definitions of incongruity and resolution are often ambiguous and leave much room for disparate interpretations across scholars [cite]. In this paper, we use a computational model of language to formalize incongruity and resolution in order to empirically evaluate their relationships to linguistic humor. While we aim to develop a model that encompasses a broad range of humorous stimuli, a critical challenge lies in the fact that complex cognitive phenomena like humor rely on rich commonsense knowledge and discourse understanding, which are challenging topics and largely unsolved in both artificial intelligence and cognitive science. To somewhat limit the scope of our task and work within the constraints of formal representations of meaning, we focus on applying formalizations of incongruity and resolution to a subset of linguistic humor: puns. In particular, we focus on phonetic puns—puns containing words that sound identical or similar to other words in the English language—because the space of possible interpretations and meaning representations of a phonetic pun is relatively constrained and well defined. An example helps to illustrate: “The magician got so mad he pulled his hare out.”This sentence allows for two interpretations:
\begin{itemize}\item[(a)] The magician got so mad he performed the trick of pulling a rabbit out of his hat.\item[(b)] The magician got so mad he (idiomatically) pulled out the hair on his head.\end{itemize}If the comprehender interprets the word “hare” as itself, he will arrive at interpretation (a); if he interprets the word as its homophone “hair,” he will arrive at interpretation (b). The sentence-level differences between interpretations (a) and (b) can thus be approximated by the two interpretations of the observed word “hare.” In general, distinct interpretations of a phonetic pun hinges on one phonetically ambiguous word, allowing the two lexical forms of the ambiguous word to stand in for competing interpretations of the entire sentence. This allows us to tackle humor without having to solve the problem of representing meaning in complex sentences or discourse.

\begin{table}[h]
\caption{Examples of identical homophone sentences}\label{identical-sentences}
\begin{tabular}{l l}
\hline
\textbf{Sentence Type} & \textbf{Example}\\
\hline
Pun & The magician was so mad he pulled is hare out. \\
Non-pun & The hare ran rapidly across the field. \\
Non-pun & Some people have lots of hair on their heads. \\
\hline
\end{tabular}
\end{table}
\begin{table}
\caption{Examples of near homophone sentences}\label{near-sentences}
\begin{tabular}{l l}
\hline
\textbf{Sentence Type} & \textbf{Example}\\
\hline
Pun & A dentist has to tell a patient the whole tooth. \\
Non-pun & A dentist examines one tooth at a time. \\
Non-pun & She always speaks the truth. \\
\hline
\end{tabular}
\end{table}Critically, even though the example we gave was a written pun and the reader sees the word ``hare" explicitly on the page, the ``hair" interpretation is still present and even salient in the context of the sentence. The possibility of interpretations that are distinct from the observed sentence itself is captured by noisy channel models of sentence processing, which posit that language comprehension is a rational process that incorporates uncertainty about surface input to arrive at sentence-level interpretations that are globally coherent [cite]. Comprehenders can thus consider multiple word-level interpretations to arrive at more than one interpretation of a sentence, each coherent but potentially incongruous with each other. The notion of incongruity fits naturally into a noisy channel model of sentence comprehension. This intuition also corresponds with that of writer and philosopher Henri Bergson, who defined a pun as “a sentence or utterance in which two ideas are expressed, and we are confronted with only one series of words."

While our approach is novel in its quantitative rigor, it is directly informed by incongruity-resolution models of humor. Since incongruity and resolution are properties of the interpretations derived from a sentence, we first describe a probabilistic model of sentence interpretation. Our model aims to infer the topic of a sentence (a coarse representation of its meaning) from the observed words. Unlike previous such models, however, we take a noisy channel approach, assuming that the comprehender maintains uncertainty over which words reflect the sentence topic and which are noise. 

\begin{figure}
\centering
\tikzset{decorate sep/.style 2 args=
{decorate,decoration={shape backgrounds,shape=circle,shape size=#1,shape sep=#2}}}
\begin{tikzpicture}
\tikzstyle{place}=[circle,draw,inner sep=2pt,minimum size=0.95cm]
 \tikzstyle{plate}=[rectangle,draw,inner sep=0pt]
 \node[place] (m) at (0,3) {$m$};
 \node[place] (w1) at (-2,1) {$w_1$};
 \node[place] (w2) at (-1,1) {$w_2$};
 \node[place] (h) at (0.5,1) {$h$}; 
 \node[place] (wn) at (2,1) {$w_n$};
 \node[place] (f1) at (-2, -0.5) {$f_1$};
 \node[place] (f2) at (-1, -0.5) {$f_2$};
\node[place] (fh) at (0.5, -0.5) {$f_h$};
\node[place] (fn) at (2, -0.5) {$f_n$};
 %\node[place] (wordsprior) at (0,4.5) {$\wordsprior$};
\draw [->] (m) -- (w1);
\draw [->] (m) -- (w2);
\draw [->] (m) -- (h);
\draw [->] (m) -- (wn);
\draw [->] (f1) -- (w1);
\draw [->] (f2) -- (w2);
\draw [->] (fh) -- (h);
\draw [->] (fn) -- (wn);
\draw[decorate sep={0.3mm}{1.65mm},fill] (-0.41,1) -- (-0.05,1);
\draw[decorate sep={0.3mm}{1.65mm},fill] (-0.41,-0.5) -- (-0.05,-0.5);
\draw[decorate sep={0.3mm}{1.65mm},fill] (1.09,1) -- (1.45,1);
\draw[decorate sep={0.3mm}{1.65mm},fill] (1.09,-0.5) -- (1.45,-0.5);
\end{tikzpicture}
\caption{Generative model of a sentence. Each word $w_i$ is generated based on the sentence topic $m$ if the indicator variable $f_i$ puts it in semantic focus; otherwise it is generated as noise (from a unigram distribution). }
\label{generativeModel}
\end{figure}

Assume our sentence is composed of a vector of content words $\vec w = \{w_1, \dots, w_i, h, w_{i+1}, \dots ,w_n\}$, including a phonetically ambiguous word $h$. We will use a simple generative model for $\vec w$ (see Figure~\ref{generativeModel}). Our model is motivated by the important roles that both semantic priming and the sequential structure of language play in lexical disambiguation during sentence processing [cite]. Given the latent sentence topic $m$, each word is generated independently by first deciding if it reflects the topic (the indicator variable $f_i$). If so it is sampled based on semantic relevance to $m$; if not it is sampled from an n-gram model that takes into account the immediately preceding words. We thus view the sentence as a mixture of topical and non-topical words. Similar approaches have been used in generative models of language to account for words that provide non-semantic information, such as topic models that incorporate syntax [cite]. We make the simplifying assumption that the plausible candidate topics $m$ of the sentence correspond to the potential interpretations of the homophone word $h$, which are constrained by phonetic similarity to two alternatives, $m_1$ and $m_2$. For example, in the magician pun described above, $h$ is the phonetically ambiguous target word ``hare," and  $m_1$ and $m_2$ are the candidate interpretations \emph{hare} and \emph{hair}. The two potential topics of the sentence can be identified by the two interpretations \emph{hare} and \emph{hair}. This assumption reduces the ill-defined space of sentence meanings to the simple proxy of alternate spellings for phonetically ambiguous words.Using the above generative model, we can infer the joint probability distribution $P(m, \vec f | \vec w) $ of the sentence topic $m$ and the indicator variables $\vec f$ that determine whether each word is in semantic focus. This distribution can be factorized into:\begin{align}P(m, \vec f | \vec w) = P(m | \vec w) P(\vec f | m, \vec w) \end{align}The two terms on the right-hand side are the basis for our derivations of measures for incongruity and resolution, respectively. Incongruity means the presence of two similarly likely interpretations, which can be quantified as a summary of the binomial distribution $P(m | \vec w)$. If the entropy of this distribution is low, then only one meaning is likely; if the entropy is high, then both meanings are similarly likely. Under the assumption that the two candidate meanings are sufficiently different from each other, high entropy in the meaning distribution suggests coexistence of two incompatible interpretations, which directly characterizes incongruity.Resolution measures the degree to which two interpretations are supported by distinct parts of the sentence. We represent this as the divergence between sets of words that are in semantic focus given the two values of $m$, which can be quantified as a summary of the distribution $P(\vec f | m, \vec w)$. We use a Kullback-Leibler divergence score $D_{KL}(F_1 || F_2) + D_{KL}(F_2 || F_1)$ to measure the distance between $F_1$ and $F_2$. This score measures how “distinct" the semantic focus sets are given $m_1$ and $m_2$. A low KL score would indicate that meanings $m_1$ and $m_2$ are supported by similar subsets of the sentence; a high KL score would indicate that $m_1$ and $m_2$ are both strongly supported by distinct subsets of the sentence. Together, these two measures constitute our formalization of humor as informed by a two-stage incongruity-resolution model. 

%------------------------------------------------

\section{Results}

We evaluate the contribution of each of our quantitative measures of incongruity and resolution to humor by correlating the measures with humans’ judgments of humor in two separate corpora of phonetically ambiguous sentences. We first evaluate our measures on $195$ sentences, in which the two candidate meanings of the phonetically ambiguous word sound identical to each other. Of these sentences, $65$ are identical homophone puns and $130$ are non-pun control sentences that match the puns in containing the same phonetically ambiguous words. To confirm that our measures generalize to broader types of ins, we also evaluate on $240$ sentences in which the two candidate meanings of the phonetically ambiguous word sound similar but not identical to each other. Of these sentences, $80$ are near homophone puns and $160$ are non-pun control sentences.

\begin{table}[h]
\centering
\begin{tabular}{l l l l}\hline
& {Estimate} & Std. Error & {p value} \\\hline
Intercept & $-0.830$ & $0.1070$ & $< 0.0001$ \\
Ambiguity & $1.899$ & $0.212$ & $< 0.0001$\\
Distinctiveness & $0.568$ & $0.082$ & $< 0.0001$\\\hline
\end{tabular}
\caption{Regression coefficients using ambiguity and distinctiveness to predict funniness ratings}
\label{coefficients}
\end{table}

Following the derivations and using the relatedness measures described in the Appendix, we computed an incongruity and distinctiveness value for each of the $435$ sentences. As predicted, incongruity differs significantly across sentence types ($F(1, 433) = 108.4, p < 0.0001 $) and correlates significantly with human ratings of funniness across all pun and non-pun sentences ($r = 0.42, p < 0.0001$). However, incongruity does not correlate with human ratings of funniness within the $145$ pun sentences ($r = 0.03, p > 0.05$). 

On the other hand, distinctiveness differs marginally significantly across sentence types ($F(1, 433) = 47.1, p < 0.01)$ and correlates significantly with human ratings of funniness across all $435$ sentences, although to a lesser extent than incongruity scores ($r = 0.35, p < 0.001$). Importantly, distinctiveness ratings correlate significantly with human ratings of funniness within the pun sentences only ($r = 0.28, p < 0.001$). This suggests that while high ambiguity distinguishes puns from non-puns, distinctiveness of the supporting context for each meaning is needed to separate very funny puns from mediocre ones.
A linear regression showed that both incongruity and resolution are significant predictors of funniness. Together, the two predictors capture a modest but significant amount of the variance in funniness ratings ($F(2,432) = 76.79,  R^2 = 0.26, p < 0.0001$; see Table~\ref{coefficients}). Using both incongruity and resolution as dimensions that formalize humor, we can distinguish among pun and non-pun sentences, as shown in Figure~\ref{ellipse}. Figure~\ref{ellipse} shows the standard error ellipses for the two sentence types in the two-dimensional space of incongruity and resolution. Although there is a fair amount of noise in the predictors (likely due to simplifying assumptions, the need to use empirical measures of relatedness, and the inherent complexity of humor) we see that pun sentences tend to cluster at a space with higher incongruity and resolution, while non-puns score significantly lower on both incongruity and resolution. 

\begin{figure}[h]
\centerline{\includegraphics[width=8.7cm]{ellipse.pdf}}
\caption{Standard error ellipses of ambiguity and distinctiveness across sentence types. Puns score higher on ambiguity and distinctiveness; non-puns have low ambiguity and distinctiveness.}\label{ellipse}
\end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=8.7cm]{scatter.pdf}}
\caption{Figure caption}\label{placeholder}
\end{figure}

\begin{table*}[!htbp]
\centering
\hfill{}
\begin{tabular}{l l l l l l l}
\hline
\textbf{$m1$}& \textbf{$m2$} & \textbf{Type} & \textbf{Sentence and Semantic Focus Sets}& \textbf{Amb.} & \textbf{Disj.} & \textbf{Funniness}\\\hline
\multirow{4}{*}{{\color{Red} hare}} & \multirow{4}{*}{{\color{Emerald} hair}} & Pun & The \textbf{{\color{Red}magician}} got so mad he \textbf{{\color{Emerald}pulled}} his \textbf{{\color{Red}hare}} out. & $0.570$ & $3.405$ & $1.714$\\ 
&&De-pun & The professor got so mad he \textbf{{\color{Emerald} pulled}} his \textbf{{\color{Red} hare}} out. & $0.575$ & $2.698$ & $0.328$\\
&&Non-pun & The \textbf{{\color{Red} hare}} \textbf{{\color{Red} ran}} \textbf{{\color{Red} rapidly}} through the \textbf{{\color{Red} fields}}. & $0.055$ &	$2.791$ &	$-0.400$\\
&&Non-pun & Most \textbf{{\color{Emerald} people}} have \textbf{{\color{Emerald} lots}} of \textbf{{\color{Emerald} hair}} on their \textbf{{\color{Emerald} heads}}. & $2.76E^{-5}$ &	$3.920$ & $-0.343$ \\\hline

\multirow{4}{*}{{\color{Red} tiers}} & \multirow{4}{*}{{\color{Emerald} tears}} & Pun & It was an \textbf{{\color{Emerald} emotional}} \textbf{{\color{NavyBlue} wedding}}. Even the \textbf{{\color{Red} cake}} was in \textbf{{\color{Red} tiers.}} & $0.333$ & $3.424$ & $1.541$\\
&&De-pun & It was an \textbf{{\color{Emerald} emotional}} \textbf{{\color{NavyBlue} wedding}}. Even the \textbf{{\color{Emerald} mother-in-law}} was in \textbf{{\color{Red} tiers}}. & $0.693$ & $2.916$ &	$0.057$ \\

&&Non-pun & \textbf{{\color{Red} Boxes}} are \textbf{{\color{Red}stacked}} in \textbf{{\color{Red} tiers}} in the warehouse. & $0.018$ & 	$3.203$ &	$-0.560$\\

&&Non-pun & \textbf{{\color{Emerald} Tears}} ran down her \textbf{{\color{Emerald} cheeks}} as she watched a \textbf{{\color{Emerald} sad}} \textbf{{\color{Emerald} movie}}.& $1.73E^{-5}$	& $4.397$ &	$-0.569$ \\
\hline
\end{tabular}
\hfill{}
\caption{Semantic focus sets, ambiguity/disjointedness scores, and funniness ratings for two groups of sentences. Words in red are in semantic focus with $m_1$; green with $m_2$; blue with both. Semantic focus sets for all sentences can be found at \url{http://www.stanford.edu/~justinek/Pun/focusSets.html}}
\label{focusExamples}
\end{table*}


\subsection{Simulations}

\subsubsection{Simulation 1}

\subsubsection{Simulation 2}

\subsection{Real Data}

%------------------------------------------------

\section{Discussion}

We believe our work represents a step towards developing models of language that can capture rich social and linguistic meaning. From the perspective of language understanding, such phenomena can serve as probes for developing models of language that account for the subtleties of linguistic behavior. From the perspective of humor research, such computational models allow for formalizations that can help empirically validate and refine existing theories. We hope that our work contributes to research in humor theory, computational humor, and language understanding, with the aim to one day understand what makes us laugh and build robots that appreciate the wonders of word play.

%----------------------------------------------------------------------------------------
%	MATERIALS AND METHODS
%----------------------------------------------------------------------------------------

%% Optional Materials and Methods Section
%% The Materials and Methods section header will be added automatically.

\begin{materials}
Our first dataset consists of $40$ identical homophone pun sentences from a website called ``Pun of the Day" (\url{http://www.punoftheday.com/}). Puns were selected such that the ambiguous item in each pun is a single phonetically ambiguous word. To obtain more identical homophone pun items, a research assistant generated an additional $25$ pun sentences based on a separate list of homophone words. We then selected $130$ corresponding non-pun sentences from an online version of Heinle's Newbury House Dictionary of American English (\url{http://nhd.heinle.com/}). We chose sample sentences included in the definition of the homophone word. $65$ of the sentences contain the ambiguous words from the pun sentences, and $65$ of them contain the alternative homophones. This design ensured that puns and non-pun sentences contain the same set of phonetically ambiguous words. Table~\ref{identical-sentences} shows example sentences from each category. Our second dataset consists of $80$ near homophone pun sentences from the same pun website, as well as $160$ corresponding near homophone non-pun sentences.

%% fill in for near homophone puns
We obtained funniness ratings for the two datasets. The$195$ identical homophone sentences were rated by $100$ subjects on Amazon's Mechanical Turk. Each subject read roughly $60$ sentences in random order, counterbalanced for the sentence types, and rated each sentence on funniness and correctness. The average split-half correlation of funniness ratings was $0.83$. Figure~\ref{ratings_analyses} shows the average funniness ratings of puns and non-pun sentences. Pun sentences are rated as significantly funnier than non-pun sentences ($F(2, 232) = 415.3, p < 0.0001$).

As described in the model section, computing ambiguity and distinctiveness measures requires the prior probabilities of meanings $P(m)$ (approximated as the unigram probabilities of the words that denote the meanings), the prior probabilities of words $P(w)$, and the conditional probabilities of each word in the sentence given a meaning $P(w | m)$. While we computed $P(w)$ and $P(m)$ directly from the Google Web unigram corpus, $P(w | m)$ is difficult to obtain through traditional topic models trained on corpora due to data sparsity. Since each meaning we consider has a single word as proxy, we may approximate $P(w | m)$ using an empirical measure of the semantic relatedness between $w$ and $m$, denoted $R(c, m)$. We use $R(c, m)$ as a proxy for point wise mutual information between $c$ and $m$, defined as follows:
\begin{align}R(w, m)= {\log \frac{P(w, m)}{P(w)P(m)}} = \log P(w | m) - \log P(w)
\end{align}We assume that human ratings of relatedness between two words $R'(w, m)$ approximate true relatedness up to an additive constant $z$. With the proper substitutions and transformations, \begin{align}P(w | m) = e^{R'(w, m) + z} P(w)\end{align}To obtain $R'(w, m)$ for each of the words $w$ in the stimuli sentences, we recruited $200$ subjects on Amazon's Mechanical Turk to rate distinct word pairs on their semantic relatedness. Since it is difficult to obtain the relatedness rating of a word with itself, we used a free parameter $r$ and fit it to data. Function words were removed from each of the sentences in our dataset, and the remaining words were paired with each of the interpretations of the homophone sequence (e.g., for the pun in Table~\ref{identical-sentences}, ``magician" and ``hare" is a legitimate word pair, as well as ``magician" and ``hair"). This resulted in $1460$ distinct word pairs. Each subject saw $146$ pairs of words in random order and were asked to rate how related each word pair is using a scale from $1$ to $10$. The average split-half correlation of the relatedness ratings was $0.916$, indicating that semantic relatedness was a reliable measure. 


\begin{definition}
\end{definition}


\begin{theorem}
\end{theorem}

\end{materials}

%----------------------------------------------------------------------------------------
%	APPENDICES (OPTIONAL)
%----------------------------------------------------------------------------------------

\appendix
An appendix without a title.

\appendix[Appendix title]
An appendix with a title.

%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

\begin{acknowledgments}
This work was partially supported by a grant from the Spanish Ministry of Science and Technology.
\end{acknowledgments}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%% PNAS does not support submission of supporting .tex files such as BibTeX.
%% Instead all references must be included in the article .tex document. 
%% If you currently use BibTeX, your bibliography is formed because the 
%% command \verb+\bibliography{}+ brings the <filename>.bbl file into your
%% .tex document. To conform to PNAS requirements, copy the reference listings
%% from your .bbl file and add them to the article .tex file, using the
%% bibliography environment described above.  

%%  Contact pnas@nas.edu if you need assistance with your
%%  bibliography.

% Sample bibliography item in PNAS format:
%% \bibitem{in-text reference} comma-separated author names up to 5,
%% for more than 5 authors use first author last name et al. (year published)
%% article title  {\it Journal Name} volume #: start page-end page.
%% ie,
% \bibitem{Neuhaus} Neuhaus J-M, Sitcher L, Meins F, Jr, Boller T (1991) 
% A short C-terminal sequence is necessary and sufficient for the
% targeting of chitinases to the plant vacuole. 
% {\it Proc Natl Acad Sci USA} 88:10362-10366.


%% Enter the largest bibliography number in the facing curly brackets
%% following \begin{thebibliography}

\begin{thebibliography}{10}
\bibitem{BN}
M.~Belkin and P.~Niyogi, {\em Using manifold structure for partially
  labelled classification}, Advances in NIPS, 15 (2003).

\bibitem{BBG:EmbeddingRiemannianManifoldHeatKernel}
P.~B\'erard, G.~Besson, and S.~Gallot, {\em Embedding {R}iemannian
  manifolds by their heat kernel}, Geom. and Fun. Anal., 4 (1994),
  pp.~374--398.

\bibitem{CLAcha1}
R.R.~Coifman and S.~Lafon, {\em Diffusion maps}, Appl. Comp. Harm. Anal.,
  21 (2006), pp.~5--30.

\bibitem{DiffusionPNAS}
R.R.~Coifman, S.~Lafon, A.~Lee, M.~Maggioni, B.~Nadler, F.~Warner, and
  S.~Zucker, {\em Geometric diffusions as a tool for harmonic analysis and
  structure definition of data. {P}art {I}: Diffusion maps}, Proc. of Nat.
  Acad. Sci.,  (2005), pp.~7426--7431.

\bibitem{Clementi:LowDimensionaFreeEnergyLandscapesProteinFolding}
P.~Das, M.~Moll, H.~Stamati, L.~Kavraki, and C.~Clementi, {\em
  Low-dimensional, free-energy landscapes of protein-folding reactions by
  nonlinear dimensionality reduction}, P.N.A.S., 103 (2006), pp.~9885--9890.

\bibitem{DoGri}
D.~Donoho and C.~Grimes, {\em Hessian eigenmaps: new locally linear
  embedding techniques for high-dimensional data}, Proceedings of the National
  Academy of Sciences, 100 (2003), pp.~5591--5596.

\bibitem{DoGri:WhenDoesIsoMap}
D.~L. Donoho and C.~Grimes, {\em When does isomap recover natural
  parameterization of families of articulated images?}, Tech. Report Tech. Rep.
  2002-27, Department of Statistics, Stanford University, August 2002.

\bibitem{GruterWidman:GreenFunction}
M.~Gr\"uter and K.-O. Widman, {\em The {G}reen function for uniformly
  elliptic equations}, Man. Math., 37 (1982), pp.~303--342.

\bibitem{Simon:NeumannEssentialSpectrum}
R.~Hempel, L.~Seco, and B.~Simon, {\em The essential spectrum of neumann
  laplacians on some bounded singular domains}, 1991.

\bibitem{1}
Kadison, R.\ V.\ and Singer, I.\ M.\ (1959)
Extensions of pure states, {\it Amer.\ J.\ Math.\ \bf
81}, 383-400.

\bibitem{2}
Anderson, J.\ (1981) A conjecture concerning the pure states of
$B(H)$ and a related theorem. in {\it Topics in Modern Operator
Theory}, Birkha\"user, pp.\ 27-43.

\bibitem{3}
Anderson, J.\ (1979) Extreme points in sets of
positive linear maps on $B(H)$. {\it J.\ Funct.\
Anal.\
\bf 31}, 195-217.

\bibitem{4}
Anderson, J.\ (1979) Pathology in the Calkin algebra. {\it J.\
Operator Theory \bf 2}, 159-167.

\bibitem{5}
Johnson, B.\ E.\ and Parrott, S.\ K.\ (1972) Operators commuting
with a von Neumann algebra modulo the set of compact operators.
{\it J.\ Funct.\ Anal.\ \bf 11}, 39-61.

\bibitem{6}
Akemann, C.\ and Weaver, N.\ (2004) Consistency of a
counterexample to Naimark's problem. {\it Proc.\ Nat.\ Acad.\
Sci.\ USA \bf 101}, 7522-7525.

\bibitem{TSL}
J.~Tenenbaum, V.~de~Silva, and J.~Langford, {\em A global geometric
  framework for nonlinear dimensionality reduction}, Science, 290 (2000),
  pp.~2319--2323.

\bibitem{ZhaZha}
Z.~Zhang and H.~Zha, {\em Principal manifolds and nonlinear dimension
  reduction via local tangent space alignement}, Tech. Report CSE-02-019,
  Department of computer science and engineering, Pennsylvania State
  University, 2002.
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{article}

%----------------------------------------------------------------------------------------
%	FIGURES AND TABLES
%----------------------------------------------------------------------------------------

%% Adding Figure and Table References
%% Be sure to add figures and tables after \end{article}
%% and before \end{document}

%% For figures, put the caption below the illustration.
%%
%% \begin{figure}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure}

%% For Tables, put caption above table
%%
%% Table caption should start with a capital letter, continue with lower case
%% and not have a period at the end
%% Using @{\vrule height ?? depth ?? width0pt} in the tabular preamble will
%% keep that much space between every line in the table.

%% \begin{table}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
%% table text
%% \end{tabular}
%% \end{table}

%% For two column figures and tables, use the following:

%% \begin{figure*}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure*}

%% \begin{table*}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{ccc}
%% table text
%% \end{tabular}
%% \end{table*}

%----------------------------------------------------------------------------------------

\end{document}