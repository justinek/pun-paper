%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proceedings of the National Academy of Sciences (PNAS)
% LaTeX Template
% Version 1.0 (19/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% The PNAStwo class was created and is owned by PNAS:
% http://www.pnas.org/site/authors/LaTex.xhtml
% This template has been modified from the blank PNAS template to include
% examples of how to insert content and drastically change commenting. The
% structural integrity is maintained as in the original blank template.
%
% Original header:
%% PNAStmpl.tex
%% Template file to use for PNAS articles prepared in LaTeX
%% Version: Apr 14, 2008
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

%------------------------------------------------
% BASIC CLASS FILE
%------------------------------------------------

%% PNAStwo for two column articles is called by default.
%% Uncomment PNASone for single column articles. One column class
%% and style files are available upon request from pnas@nas.edu.

%\documentclass{pnasone}
\documentclass{pnastwo}

%------------------------------------------------
% POSITION OF TEXT
%------------------------------------------------

%% Changing position of text on physical page:
%% Since not all printers position
%% the printed page in the same place on the physical page,
%% you can change the position yourself here, if you need to:

% \advance\voffset -.5in % Minus dimension will raise the printed page on the 
                         %  physical page; positive dimension will lower it.

%% You may set the dimension to the size that you need.

%------------------------------------------------
% GRAPHICS STYLE FILE
%------------------------------------------------

%% Requires graphics style file (graphicx.sty), used for inserting
%% .eps/image files into LaTeX articles.
%% Note that inclusion of .eps files is for your reference only;
%% when submitting to PNAS please submit figures separately.

%% Type into the square brackets the name of the driver program 
%% that you are using. If you don't know, try dvips, which is the
%% most common PC driver, or textures for the Mac. These are the options:

% [dvips], [xdvi], [dvipdf], [dvipdfm], [dvipdfmx], [pdftex], [dvipsone],
% [dviwindo], [emtex], [dviwin], [pctexps], [pctexwin], [pctexhp], [pctex32],
% [truetex], [tcidvi], [vtex], [oztex], [textures], [xetex]


%------------------------------------------------
% OPTIONAL POSTSCRIPT FONT FILES
%------------------------------------------------

%% PostScript font files: You may need to edit the PNASoneF.sty
%% or PNAStwoF.sty file to make the font names match those on your system. 
%% Alternatively, you can leave the font style file commands commented out
%% and typeset your article using the default Computer Modern 
%% fonts (recommended). If accepted, your article will be typeset
%% at PNAS using PostScript fonts.

% Choose PNASoneF for one column; PNAStwoF for two column:
%\usepackage{PNASoneF}
%\usepackage{PNAStwoF}

%------------------------------------------------
% ADDITIONAL OPTIONAL STYLE FILES
%------------------------------------------------

%% The AMS math files are commonly used to gain access to useful features
%% like extended math fonts and math commands.
\usepackage{multirow}
%\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{etex}
\usepackage{color}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{decorations.shapes}
\usepackage{amssymb,amsfonts,amsmath}

%------------------------------------------------
% OPTIONAL MACRO FILES
%------------------------------------------------

%% Insert self-defined macros here.
%% \newcommand definitions are recommended; \def definitions are supported

%\newcommand{\mfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
%\def\s{\sigma}

%------------------------------------------------
% DO NOT EDIT THIS SECTION
%------------------------------------------------

%% For PNAS Only:
\contributor{Submitted to Proceedings of the National Academy of Sciences of the United States of America}
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHORS
%----------------------------------------------------------------------------------------

\title{A computational model of linguistic humor} % For titles, only capitalize the first letter

%------------------------------------------------

%% Enter authors via the \author command.  
%% Use \affil to define affiliations.
%% (Leave no spaces between author name and \affil command)

%% Note that the \thanks{} command has been disabled in favor of
%% a generic, reserved space for PNAS publication footnotes.

%% \author{<author name>
%% \affil{<number>}{<Institution>}} One number for each institution.
%% The same number should be used for authors that
%% are affiliated with the same institution, after the first time
%% only the number is needed, ie, \affil{number}{text}, \affil{number}{}
%% Then, before last author ...
%% \and
%% \author{<author name>
%% \affil{<number>}{}}

%% For example, assuming Garcia and Sonnery are both affiliated with
%% Universidad de Murcia:
%% \author{Roberta Graff\affil{1}{University of Cambridge, Cambridge,
%% United Kingdom},
%% Javier de Ruiz Garcia\affil{2}{Universidad de Murcia, Bioquimica y Biologia
%% Molecular, Murcia, Spain}, \and Franklin Sonnery\affil{2}{}}

\author{Justine T. Kao\affil{1}{Stanford University},
Roger Levy\affil{2}{University of California, San Diego}
\and
Noah D. Goodman\affil{1}{}}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

%----------------------------------------------------------------------------------------

\maketitle % The \maketitle command is necessary to build the title page

\begin{article}

%----------------------------------------------------------------------------------------
%	ABSTRACT, KEYWORDS AND ABBREVIATIONS
%----------------------------------------------------------------------------------------

\begin{abstract}
Humor plays an essential role in human interactions. However, its precise nature remains elusive. While research in natural language understanding has made significant advancements in recent years, there has been little direct integration of humor research with computational models of general language understanding. In this paper, we propose two information-theoretic measures of humor---ambiguity and distinctiveness---inspired by humor theories and directly derived from a simple model of sentence processing. We then test these measures on a set of puns and non-pun sentences and show that they correlate significantly with human judgments of funniness. Our model is one of the first to integrate general linguistic knowledge, speaker intent, and humor theory to model humor computationally. We present it as a framework for applying models of language processing to understand higher-level linguistic and cognitive phenomena.

\end{abstract}

%------------------------------------------------

\keywords{Linguistic humor | Language understanding | Computational modeling} % When adding keywords, separate each term with a straight line: |

%------------------------------------------------

%% Optional for entering abbreviations, separate the abbreviation from
%% its definition with a comma, separate each pair with a semicolon:
%% for example:
%% \abbreviations{SAM, self-assembled monolayer; OTS,
%% octadecyltrichlorosilane}

% \abbreviations{}
\abbreviations{IR, Incongruity Resolution}

%----------------------------------------------------------------------------------------
%	PUBLICATION CONTENT
%----------------------------------------------------------------------------------------

%% The first letter of the article should be drop cap: \dropcap{} e.g.,
%\dropcap{I}n this article we study the evolution of ''almost-sharp'' fronts

\section{Introduction}

\dropcap{L}ove may make the world go round, but humor is the glue that keeps it together. Previous research has shown that humor is ubiquitous across cultures \cite{martin2010psychology, kruger1996nature}, increases interpersonal attraction \cite{lundy1998heterosexual}, helps resolve intergroup conflicts \cite{smith2000resolving}, and improves psychological wellbeing \cite{martin1993humor}. Our everyday experiences show that humor plays a critical role in human interactions and composes a significant part of our linguistic, cognitive, and social lives. However, the cognitive basis of humor remains relatively unknown. In this paper, we build upon theories of humor and language processing to computationally model the humor in a set of phonetic puns. By providing a formal model of linguistic humor, we aim to shed light on how the mind identifies and processes the inputs that make us laugh.

Of the many theories of humor proposed since Aristotle, the incongruity theory comes closest to a cognitive account. Incongruity is defined as the incompatible and often schema-violating interpretations of a stimulus. 
%As Veale (2004) states, ``Of the few sweeping generalizations one can make about humor that are neither controversial or trivially false, one is surely that humor is a phenomenon that relies on incongruity." 
While most humor researchers agree that the experience of incongruity is necessary for generating humor, some argue that it alone is insufficient. These scholars propose a two-stage model of humor termed the Incongruity-Resolution (IR) model, which involves the discovery of a cognitive rule that explains and resolves the incongruity in the stimulus. However, definitions of both incongruity and resolution are ambiguous and leave much room for disparate interpretations among humor researchers. While informal models of humor have provided important insights into the necessary and sufficient conditions of humor, the lack of computational rigor makes it difficult to operationalize these conditions or to empirically evaluate their contribution to the perception of humor.

More recently, researchers in artificial intelligence and computational linguistics have applied computational tools to examine features of humor. The interest in computational humor was motivated in part by the need for computers to recognize and generate humorous input in order to interact with humans in a more engaging and natural manner \cite{mihalcea2006learning}. However, most work on computational humor focuses either on joke-specific templates and schemata \cite{binsted1996machine, kiddon2011s} or surface features such as innuendo and slang \cite{mihalcea2006learning, semantic2010}. While these approaches are able to identify and produce humorous stimuli within certain constraints, they fall short of testing and building upon a more general cognitive theory of humor.

In this paper, we directly utilize a computational model of sentence processing to derive theory-driven measures of humor. By basing our measures of humor on existing ideas such as incongruity and resolution, we are able to leverage and test the insights generated by decades of qualitative research. By formalizing these measures, we can quantitively evaluate how different factors contribute to the experience of humor. Furthermore, by deriving these measures from a model of general sentence processing, we are able to view linguistic humor as a direct result of language understanding strategies instead of as a separate dedicated process.

While we aim to develop a model that encompasses a broad range of humorous stimuli, many types of humor rely on rich commonsense knowledge and discourse understanding. To reasonably limit the scope of our task, we focus on testing our model on a subset of linguistic humor for which we are able to obtain reasonable formal representations of meaning. In particular, we focus on phonetic puns, which are puns containing words that sound identical or similar to other words in the English language. Unlike other types of jokes, the space of possible meaning representations of a phonetic pun is relatively constrained and well defined. For example, consider the following sentence:
\begin{itemize}
\item[] ``The magician got so mad he pulled his hare out." 
\end{itemize}
This sentence allows for two different interpretations:
\begin{itemize}
\item[(a)] The magician got so mad he performed the trick of pulling a rabbit out of his hat.
\item[(b)] The magician got so mad he (idiomatically) pulled out the hair on his head.
\end{itemize}

If the comprehender interprets the word ``hare" as \emph{hare}, he will arrive at interpretation (a). if he interprets the word as the homophone \emph{hair}, however, he will arrive at interpretation (b). In general, interpretations of a phonetic pun hinge on a single phonetically ambiguous word, which allows us to use different homophones of the ambiguous word to approximate different interpretations of the entire sentence. Focusing on phonetic puns thus enables us to formalize measures of humor without first having to represent the meaning of complex sentences and discourse.

We observe that although the reader sees the word ``hare" explicitly when processing the example given, the ``hair" interpretation is still highly accessible. Previous research on sentence comprehension has suggested that people maintain uncertainty about the surface input when processing a sentence. In other words, comphrehenders assume that communication happens through a ``noisy channel" and that some parts of the input they receive may have been corrupted. In order to successfully infer the true meaning of a sentence, comprehenders consider multiple word-level interpretations during processing and rationally incorporate them to arrive at coherent interpretations at the sentence level. By positing noise in the input and modeling comprehension as rational inference under uncertainty, the noisy-channel model is able to explain a variety of phenomena in language processing. 

Here we propose that the humor in phonetic puns may also arise from the assumption of a noisy channel. Because the comprehender maintains uncertainty about the input, it is possible for her to arrive at multiple interpretations of a sentence that are each coherent but incongruous with each other. Previous research has shown that both semantic priming and the sequential structure of language play important roles in sentence processing. We propose a model of sentence comprehension that incorporates the noisy-channel assumption, the semantic relationship between a sentence's overall meaning and the words that compose it, and the sequential structure of language to infer the meaning of a sentence. 

\subsection{Model}

Suppose a sentence is composed of a vector of words $\vec w = \{w_1, \dots, w_i, h, w_{i+1}, \dots ,w_n\}$, where $h$ is a phonetically ambiguous word. We construct a simple generative model for $\vec w$ (see Figure~\ref{generativeModel}). Given a latent sentence meaning $m$, each word is generated independently by first deciding if it explicitly reflects the sentence meaning or is corrupted by noise. This process is indicated by an indicator variable vector $\vec f$, where $w_i$ is a meaningful word if $f_i = 1$ and a noise word if $f_i = 0$. When $w_i$ is a meaningful word, it is sampled in proportion to its semantic relevance to $m$. When it is a corrupted word, we assume that its corruption is affected by the immediately preceding words and sample it from an n-gram language model. This is psychologically plausible given previous work on blah blah. 

To select the sentence meanings $m$, we exploit the convenient property of phonetic puns described before. We introduce the simplifying assumption that the sentence meanings $m$ correspond to plausible interpretations of the homophone word $h$, which are constrained by phonetic similarity. For example, ``hare" is a phonetically ambiguous word, and the homophones \emph{hare} and \emph{hair} can approximate the two possible sentence meanings $m_1$ and $m_2$. While this approximation is admittedly coarse, it makes use of the reasonable assumption that sentences containing the word ``hare" will generally be about the topic \emph{hare}, and sentences that have the word ``hair" will generally be about the topic \emph{hair}. This assumption reduces the ill-defined space of sentence meanings to the simple proxy of alternate spellings for phonetically ambiguous words.

\begin{figure}
\centering
\tikzset{decorate sep/.style 2 args=
{decorate,decoration={shape backgrounds,shape=circle,shape size=#1,shape sep=#2}}}
\begin{tikzpicture}
\tikzstyle{place}=[circle,draw,inner sep=2pt,minimum size=0.95cm]
 \tikzstyle{plate}=[rectangle,draw,inner sep=0pt]
 \node[place] (m) at (0,3) {$m$};
 \node[place] (w1) at (-2,1) {$w_1$};
 \node[place] (w2) at (-1,1) {$w_2$};
 \node[place] (h) at (0.5,1) {$h$}; 
 \node[place] (wn) at (2,1) {$w_n$};
 \node[place] (f1) at (-2, -0.5) {$f_1$};
 \node[place] (f2) at (-1, -0.5) {$f_2$};
\node[place] (fh) at (0.5, -0.5) {$f_h$};
\node[place] (fn) at (2, -0.5) {$f_n$};
 %\node[place] (wordsprior) at (0,4.5) {$\wordsprior$};
\draw [->] (m) -- (w1);
\draw [->] (m) -- (w2);
\draw [->] (m) -- (h);
\draw [->] (m) -- (wn);
\draw [->] (f1) -- (w1);
\draw [->] (f2) -- (w2);
\draw [->] (fh) -- (h);
\draw [->] (fn) -- (wn);
\draw[decorate sep={0.3mm}{1.65mm},fill] (-0.41,1) -- (-0.05,1);
\draw[decorate sep={0.3mm}{1.65mm},fill] (-0.41,-0.5) -- (-0.05,-0.5);
\draw[decorate sep={0.3mm}{1.65mm},fill] (1.09,1) -- (1.45,1);
\draw[decorate sep={0.3mm}{1.65mm},fill] (1.09,-0.5) -- (1.45,-0.5);
\end{tikzpicture}
\caption{Generative model of a sentence. Each word $w_i$ is generated based on the sentence topic $m$ if the indicator variable $f_i$ puts it in semantic focus; otherwise it is generated as noise (from a unigram distribution). }
\label{generativeModel}
\end{figure}

Using the generative model described, we can infer the joint probability distribution $P(m, \vec f | \vec w) $ of the sentence topic $m$ and the indicator variables $\vec f$ given a set of words $\vec w$. This distribution can be factorized as the following:

\begin{align}
P(m, \vec f | \vec w) = P(m | \vec w) P(\vec f | m, \vec w) 
\end{align}

We derive two formal measure of humor from the terms on the righthand side, which we call ambiguity and distinctiveness. Ambiguity captures the degree to which sentence meanings are similarly likely. This can be quantified as a summary of the binomial distribution $P(m | \vec w)$. If the entropy of this distribution is low, then one meaning is much more likely than the other. If the entropy is high, then both meanings are similarly likely. 

Distinctiveness measures the degree to which two sentence meanings are supported by distinct parts of the sentence. This can be quantified as a summary of the distribution $P(\vec f | m, \vec w)$. Given $\vec w$ and the topic $m_1$, which is directly observed in the sentence, we compute the distribution $F_1 = P(\vec f | m_1, \vec w)$. Given $\vec w$ and the topic $m_2$, we compute the distribution $F_2 = P(\vec f | m_2, \vec w)$. We wish to measure how different the distribution over topic words would be given different sentence meanings. We use a Kullback-Leibler divergence score $D_{KL}(F_1 || F_2)$ to measure the distance between $F_1$ and $F_2$. A low KL score indicates that the possible sentence meanings are supported by similar subsets of the sentence. A high KL score indicates the sentence meanings are each strongly supported by distinct subsets of the sentence. 
%Note that this measure is not symmetric between $F_1$ and $F_2$. Since $m_1$ is the word that is explicitly observed in the sentence (e.g. \emph{hare}), the relationship between the two distributions is inherently asymmetric. 
Together, our ambiguity and distinctiveness measures constitute a two-dimensional formalization of humor. 

While only loosely based on existing definitions of ``incongruity" and ``resolution," the two formal measures we derived have natural connections to the IR model of humor. Under the assumption that the two sentence meanings are sufficiently different from each other, a high ambiguity score suggests the coexistence of two incompatible interpretations, which is a characterization of ``incongruity." A high distinctiveness score suggests that given one sentence meaning, the comprehender should regard one set of words as meaningful words, while given another sentence meaning, attention should be directed to a different set of words. This allows the comprehender to pay attention to different sets of words given different candidate sentence meanings, effectively ``resolving" the incongruity by discovering the cognitive rule that partitions the sentence into distinct parts that are each internally harmonious.
%------------------------------------------------

\section{Results}


We evaluated the ambiguity and distinctiveness measures on a set of $435$ phonetically ambiguous sentences. Of these sentences, $65$ are identical homophone puns and $80$ are puns where the two candidate meanings sound similar but are not identical to each other (near homophone puns). The remaining $290$ sentences are non-pun control sentences that contain the same phonetically ambiguous words as the puns. Table \ref{sentences} shows an example of each type of sentence.

\begin{table}[h]
%\caption{Example sentences}
\label{sentences}
\begin{tabular}{l l l}
\hline
\textbf{Type} & \textbf{Hom} & \textbf{Example}\\
\hline
Pun & Identical & The magician was so mad he pulled is hare out. \\
Pun & Near & A dentist has to tell a patient the whole tooth. \\
Non-pun & Identical & The hare ran rapidly across the field. \\
Non-pun & Identical & Some people have lots of hair on their heads. \\
Non-pun & Near & A dentist examines one tooth at a time. \\
Non-pun & Near & She always speaks the truth. \\
\hline
\end{tabular}
\end{table}

We obtained human ratings of funniness for each of the $435$ sentences (see Materials and Methods). Applying the derivations and relatedness measures described in the Methods section, we computed an ambiguity and distinctiveness score for each sentence. As predicted, ambiguity was significantly higher for pun sentences than non-pun sentences ($F(1, 433) = 108.4, p < 0.0001$). This suggests that our ambiguity measure successfully captures characteristics that distinguish puns from other phonetically ambiguous sentences. Intuitively, while non-pun sentences also contain phonetically ambiguous words, their interpretations are less ambiguous because the words are highly semantically related to only one sentence meaning. Our measure of distinctiveness differs marginally significantly across sentence types ($F(1, 433) = 47.1, p < 0.1)$ Using both ambiguity and distinctiveness as dimensions that formalize humor, we can distinguish among pun and non-pun sentences, as shown in Figure~\ref{ellipse}. Figure~\ref{ellipse} shows the standard error ellipses for the two sentence types in the two-dimensional space of ambiguity and distinctiveness. Although there is a fair amount of noise in the predictors (likely due to simplifying assumptions, the need to use empirical measures of relatedness, and the inherent complexity of humor), we see that pun sentences tend to cluster at a space with higher ambiguity and distinctiveness, while non-pun sentences score significantly lower on both measures. A linear regression model showed that both ambiguity and distinctiveness are significant predictors of funniness ratings across all $435$ sentences. Together, the two predictors capture a modest but significant amount of the reliable variance in funniness ratings ($F(2,432) = 76.79,  r = 0.51, p < 0.0001$; see Table~\ref{coefficients}). 

\begin{figure}[t]
\centerline{\includegraphics[width=8.6cm]{ellipse.pdf}}
\caption{Standard error ellipses of ambiguity and distinctiveness across sentence types. Puns score higher on ambiguity and distinctiveness; non-puns have low ambiguity and distinctiveness.}\label{ellipse}
\end{figure}

\begin{figure}[ht]
\centerline{\includegraphics[width=8.6cm]{scatter.pdf}}
\caption{Figure caption}\label{placeholder}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{l l l l}\hline
& {Estimate} & Std. Error & {p value} \\\hline
Intercept & $-0.830$ & $0.1070$ & $< 0.0001$ \\
Ambiguity & $1.899$ & $0.212$ & $< 0.0001$\\
Distinctiveness & $0.568$ & $0.082$ & $< 0.0001$\\\hline
\end{tabular}
%\caption{Regression coefficients using ambiguity and distinctiveness to predict funniness ratings}
\label{coefficients}
\end{table}

We now examine the measures' quantitative predictions of funniness within puns. Ambiguity does not correlate with human ratings of funniness within the $145$ pun sentences ($r = 0.03, p > 0.05$), suggesting that it alone is unable to distinguish highly funny puns from mediocre puns. On the other hand, distinctiveness ratings correlate significantly with human ratings of funniness within pun sentences ($r = 0.28, p < 0.001$). This suggests that while ambiguity distinguishes puns from non-puns, distinctiveness is needed to separate very funny puns from mediocre ones.

Besides predicting the funniness of a sentence, our model can also tell us which particular features of a pun make it amusing. By finding the most likely sets of meaningful words given each latent sentence meaning $m$ and $\vec w$, we can identify words in a pun that are critical to producing its humor. Table~\ref{focusExamples} shows the most likely sets of meaningful words given each meaning for two groups of sentences. Sentences in each group contain the same pair of candidate meanings for the target word $h$. However, they differ in measures of ambiguity, distinctiveness, and funniness.  Words in the most likely sets given $m_1$ are in red; words in the most likely sets given $m_2$ are in green; and words in the most likely  sets of both meanings are in dark blue. We observe that visually, the two pun sentences (which are significantly funnier) have more distinctive and balanced sets of meaningful words for each sentence meaning than other sentences in their groups. Non-pun sentences tend to have no words in support of the meaning that was not observed. Moreover, imagine if you were asked to explain why the two pun sentences are funny. The colorful words in each pun sentence---for example, the fact that magicians tend to perform magic tricks with hares, and people tend to be described as pulling out their hair when angry---are what one might intuitively use to explain why the sentence is a pun. Our model thus provides a natural way of not only using ambiguity and distinctiveness to predict when a sentence is a pun, but also to explain what aspects of a pun make it funny. 

%------------------------------------------------

\section{Discussion}
In this paper, we showed that rich social and linguistic meaning can arise from a basic model of sentence comprehension. 
%We described a probabilistic model of sentence comprehension that uses noisy channel assumptions to compute the meaning of a phonetically ambiguous word. 
Although our task in this paper is limited in scope, it represents a step towards developing models of language that can explain complex phenomena such as humor. From the perspective of language understanding, such phenomena can serve as probes for developing models of language processing that account for a wider range of linguistic behavior. We hope that our work contributes to research in humor theory, computational humor, and language understanding, such that some day we can understand what makes us laugh and build robots that appreciate the wonders of word play.

\begin{table*}[t]
\centering
\hfill{}
\begin{tabular}{l l l l l l l}
\hline
\textbf{$m1$}& \textbf{$m2$} & \textbf{Type} & \textbf{Sentence and Semantic Focus Sets}& \textbf{Amb.} & \textbf{Disj.} & \textbf{Funniness}\\\hline
\multirow{4}{*}{{\color{Red} hare}} & \multirow{4}{*}{{\color{Emerald} hair}} & Pun & The \textbf{{\color{Red}magician}} got so mad he \textbf{{\color{Emerald}pulled}} his \textbf{{\color{Red}hare}} out. & $0.570$ & $3.405$ & $1.714$\\ 
&&De-pun & The professor got so mad he \textbf{{\color{Emerald} pulled}} his \textbf{{\color{Red} hare}} out. & $0.575$ & $2.698$ & $0.328$\\
&&Non-pun & The \textbf{{\color{Red} hare}} \textbf{{\color{Red} ran}} \textbf{{\color{Red} rapidly}} through the \textbf{{\color{Red} fields}}. & $0.055$ &	$2.791$ &	$-0.400$\\
&&Non-pun & Most \textbf{{\color{Emerald} people}} have \textbf{{\color{Emerald} lots}} of \textbf{{\color{Emerald} hair}} on their \textbf{{\color{Emerald} heads}}. & $2.76E^{-5}$ &	$3.920$ & $-0.343$ \\\hline

\multirow{4}{*}{{\color{Red} tiers}} & \multirow{4}{*}{{\color{Emerald} tears}} & Pun & It was an \textbf{{\color{Emerald} emotional}} \textbf{{\color{NavyBlue} wedding}}. Even the \textbf{{\color{Red} cake}} was in \textbf{{\color{Red} tiers.}} & $0.333$ & $3.424$ & $1.541$\\
&&De-pun & It was an \textbf{{\color{Emerald} emotional}} \textbf{{\color{NavyBlue} wedding}}. Even the \textbf{{\color{Emerald} mother-in-law}} was in \textbf{{\color{Red} tiers}}. & $0.693$ & $2.916$ &	$0.057$ \\

&&Non-pun & \textbf{{\color{Red} Boxes}} are \textbf{{\color{Red}stacked}} in \textbf{{\color{Red} tiers}} in the warehouse. & $0.018$ & 	$3.203$ &	$-0.560$\\

&&Non-pun & \textbf{{\color{Emerald} Tears}} ran down her \textbf{{\color{Emerald} cheeks}} as she watched a \textbf{{\color{Emerald} sad}} \textbf{{\color{Emerald} movie}}.& $1.73E^{-5}$	& $4.397$ &	$-0.569$ \\
\hline
\end{tabular}
\hfill{}
%\caption{Semantic focus sets, ambiguity/disjointedness scores, and funniness ratings for two groups of sentences. Words in red are in semantic focus with $m_1$; green with $m_2$; blue with both. 
%Semantic focus sets for all sentences can be found at \url{http://www.stanford.edu/~justinek/Pun/focusSets.html}
%}
\label{focusExamples}
\end{table*}
%----------------------------------------------------------------------------------------
%	MATERIALS AND METHODS
%----------------------------------------------------------------------------------------

%% Optional Materials and Methods Section
%% The Materials and Methods section header will be added automatically.

\begin{materials}
Our first dataset consists of $40$ identical homophone pun sentences from a website called ``Pun of the Day" (\url{http://www.punoftheday.com/}). Puns were selected such that the ambiguous item in each pun is a single phonetically ambiguous word. To obtain more identical homophone pun items, a research assistant generated an additional $25$ pun sentences based on a separate list of homophone words. We then selected $130$ corresponding non-pun sentences from an online version of Heinle's Newbury House Dictionary of American English (\url{http://nhd.heinle.com/}). We chose sample sentences included in the definition of the homophone word. $65$ of the sentences contain the ambiguous words from the pun sentences, and $65$ of them contain the alternative homophones. This design ensured that puns and non-pun sentences contain the same set of phonetically ambiguous words. Table~\ref{identical-sentences} shows example sentences from each category. Our second dataset consists of $80$ near homophone pun sentences from the same pun website, as well as $160$ corresponding near homophone non-pun sentences.

% fill in for near homophone puns
We obtained funniness ratings for the two datasets. The$195$ identical homophone sentences were rated by $100$ subjects on Amazon's Mechanical Turk. Each subject read roughly $60$ sentences in random order, counterbalanced for the sentence types, and rated each sentence on funniness and correctness. The average split-half correlation of funniness ratings was $0.83$. Figure~\ref{ratings_analyses} shows the average funniness ratings of puns and non-pun sentences. Pun sentences are rated as significantly funnier than non-pun sentences ($F(2, 232) = 415.3, p < 0.0001$).

As described in the model section, computing ambiguity and distinctiveness measures requires the prior probabilities of meanings $P(m)$ (approximated as the unigram probabilities of the words that denote the meanings), the prior probabilities of words $P(w)$, and the conditional probabilities of each word in the sentence given a meaning $P(w | m)$. While we computed $P(w)$ and $P(m)$ directly from the Google Web unigram corpus, $P(w | m)$ is difficult to obtain through traditional topic models trained on corpora due to data sparsity. 
Since each meaning we consider has a single word as proxy, we may approximate $P(w | m)$ using an empirical measure of the semantic relatedness between $w$ and $m$, denoted $R(c, m)$. We use $R(c, m)$ as a proxy for point wise mutual information between $c$ and $m$, defined as follows:
\begin{align}
R(w, m)= {\log \frac{P(w, m)}{P(w)P(m)}} = \log P(w | m) - \log P(w)
\end{align}
We assume that human ratings of relatedness between two words $R'(w, m)$ approximate true relatedness up to an additive constant $z$. With the proper substitutions and transformations, 
\begin{align}
P(w | m) = e^{R'(w, m) + z} P(w)
\end{align}

To obtain $R'(w, m)$ for each of the words $w$ in the stimuli sentences, we recruited $200$ subjects on Amazon's Mechanical Turk to rate distinct word pairs on their semantic relatedness. Since it is difficult to obtain the relatedness rating of a word with itself, we used a free parameter $r$ and fit it to data. Function words were removed from each of the sentences in our dataset, and the remaining words were paired with each of the interpretations of the homophone sequence (e.g., for the pun in Table~\ref{identical-sentences}, ``magician" and ``hare" is a legitimate word pair, as well as ``magician" and ``hair"). This resulted in $1460$ distinct word pairs. Each subject saw $146$ pairs of words in random order and were asked to rate how related each word pair is using a scale from $1$ to $10$. The average split-half correlation of the relatedness ratings was $0.916$, indicating that semantic relatedness was a reliable measure. 


\end{materials}

%----------------------------------------------------------------------------------------
%	APPENDICES (OPTIONAL)
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

\begin{acknowledgments}
This work was partially supported by a grant from the Spanish Ministry of Science and Technology.
\end{acknowledgments}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%% PNAS does not support submission of supporting .tex files such as BibTeX.
%% Instead all references must be included in the article .tex document. 
%% If you currently use BibTeX, your bibliography is formed because the 
%% command \verb+\bibliography{}+ brings the <filename>.bbl file into your
%% .tex document. To conform to PNAS requirements, copy the reference listings
%% from your .bbl file and add them to the article .tex file, using the
%% bibliography environment described above.  

%%  Contact pnas@nas.edu if you need assistance with your
%%  bibliography.

% Sample bibliography item in PNAS format:
%% \bibitem{in-text reference} comma-separated author names up to 5,
%% for more than 5 authors use first author last name et al. (year published)
%% article title  {\it Journal Name} volume #: start page-end page.
%% ie,
% \bibitem{Neuhaus} Neuhaus J-M, Sitcher L, Meins F, Jr, Boller T (1991) 
% A short C-terminal sequence is necessary and sufficient for the
% targeting of chitinases to the plant vacuole. 
% {\it Proc Natl Acad Sci USA} 88:10362-10366.


%% Enter the largest bibliography number in the facing curly brackets
%% following \begin{thebibliography}

\begin{thebibliography}{10}

\bibitem{lundy1998heterosexual,
  title={Heterosexual romantic preferences: The importance of humor and physical Äness for different types of relationships},
  author={Lundy, Duane E and Tan, Josephine and Cunningham, Michael R},
  journal={Personal Relationships},
  volume={5},
  number={3},
  pages={311--325},
  year={1998},
  publisher={Wiley Online Library}
}

\bibitem{smith2000resolving,
  title={Resolving conflict with humor in a diversity context},
  author={Smith, Wanda J and Harrington, K Vernard and Neck, Christopher P},
  journal={Journal of Managerial Psychology},
  volume={15},
  number={6},
  pages={606--625},
  year={2000},
  publisher={MCB UP Ltd}
}

\bibitem{kruger1996nature,
  title={The nature of humor in human nature: Cross-cultural commonalities},
  author={Kruger, Arnold},
  journal={Counselling Psychology Quarterly},
  volume={9},
  number={3},
  pages={235--241},
  year={1996},
  publisher={Taylor}
}


\bibitem{mobbs2003humor,
  title={Humor modulates the mesolimbic reward centers},
  author={Mobbs, Dean and Greicius, Michael D and Abdel-Azim, Eiman and Menon, Vinod and Reiss, Allan L},
  journal={Neuron},
  volume={40},
  number={5},
  pages={1041--1048},
  year={2003},
  publisher={Elsevier}
}

\bibitem{martin2010psychology,
  title={The psychology of humor: An integrative approach},
  author={Martin, Rod A},
  year={2010},
  publisher={Access Online via Elsevier}
}

\bibitem{martin1993humor,
  title={Humor, coping with stress, self-concept, and psychological well-being},
  author={Martin, Rod A and Kuiper, Nicholas A and Olinger, L and Dance, Kathryn A},
  year={1993}
}

%\bibitem{ZhaZha}
%Z.~Zhang and H.~Zha, {\em Principal manifolds and nonlinear dimension
%  reduction via local tangent space alignement}, Tech. Report CSE-02-019,
%  Department of computer science and engineering, Pennsylvania State
%  University, 2002.

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{article}

%----------------------------------------------------------------------------------------
%	FIGURES AND TABLES
%----------------------------------------------------------------------------------------

%% Adding Figure and Table References
%% Be sure to add figures and tables after \end{article}
%% and before \end{document}

%% For figures, put the caption below the illustration.
%%
%% \begin{figure}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure}

%% For Tables, put caption above table
%%
%% Table caption should start with a capital letter, continue with lower case
%% and not have a period at the end
%% Using @{\vrule height ?? depth ?? width0pt} in the tabular preamble will
%% keep that much space between every line in the table.

%% \begin{table}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{@{\vrule height 10.5pt depth4pt  width0pt}lrcccc}
%% table text
%% \end{tabular}
%% \end{table}

%% For two column figures and tables, use the following:

%% \begin{figure*}
%% \caption{Almost Sharp Front}\label{afoto}
%% \end{figure*}

%% \begin{table*}
%% \caption{Repeat length of longer allele by age of onset class}
%% \begin{tabular}{ccc}
%% table text
%% \end{tabular}
%% \end{table*}

%----------------------------------------------------------------------------------------

\end{document}